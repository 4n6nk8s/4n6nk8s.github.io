{
    "version": "https://jsonfeed.org/version/1",
    "title": "4n6 & k8s • All posts by \"kubernetes\" category",
    "description": "We are a Forensics Duo at SOter14 CTF Team & Network Engineering Students at INSAT. Enjoy our Kubernetes Articles and Write-ups!",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/06/03/Kubernetes/post/",
            "url": "http://example.com/2023/06/03/Kubernetes/post/",
            "title": "Streamlined Application Deployemet using Kubernetes",
            "date_published": "2023-06-03T16:38:55.000Z",
            "content_html": "<h1 id=\"introduction\"><a class=\"markdownIt-Anchor\" href=\"#introduction\">#</a> Introduction</h1>\n<p>As second-year engineering students, we embarked our academic group project that pushes us beyond our limits. Being self-proclaimed cloud-native and Kubernetes geeks, we decided to venture into an unfamiliar territory with an advanced project. Our goal? To unravel the mysteries of Heroku, Vercel, and On-Render, the leading PaaS solutions! And here is the twist, we were running out of time, and neither ChatGPT nor Google can help us. We are mad, and we actually did it in 4 days! Fellow adventurers, let’s get you to feel the glimpse of what we have been through in just 4 days to build our project codenamed Kli8nt for streamlined application deployments into the Kubernetes realm</p>\n<p>Fueled by the increasing demand for a unified platform that encompasses front-end hosting, backend deployment, and database management, our team leveraged our expertise in DevOps, Kubernetes, and web development to establish a robust and scalable solution that is client-friendly and K8s-able, hence the name Kli8nt. It is our way to design a mechanism that’s a proof-of-concept to the leading PaaS solutions</p>\n<h1 id=\"project-details\"><a class=\"markdownIt-Anchor\" href=\"#project-details\">#</a> Project Details  :</h1>\n<h2 id=\"high-level-architecture\"><a class=\"markdownIt-Anchor\" href=\"#high-level-architecture\">#</a> High Level Architecture</h2>\n<p>Our system consists of various interconnected components that work together seamlessly. At the front-end, users interact with the system and provide the necessary information for application deployment. The backbone of our system is the backend, which handles authentication, data storage, and serves as the communication hub with the clusters. It establishes a websocket connection to deliver real-time logs to the user. To facilitate efficient communication between the backend and clusters, we rely on RabbitMQ, a messaging system that ensures reliable information exchange. The clusters themselves take charge of the build and deployment operations, guided by custom controllers to ensure efficiency and accuracy. Lastly, Kafka, our robust data streaming platform, stores and streams application logs from the clusters to the user. Together, these components form a cohesive system that enables smooth and efficient application deployment.</p>\n<p><img data-src=\"https://i.imgur.com/fEOyFeQ.png\" alt=\"\"></p>\n<h2 id=\"user-interface\"><a class=\"markdownIt-Anchor\" href=\"#user-interface\">#</a> User Interface</h2>\n<p>The User Interface serves as the primary interaction point for developers using the<br>\nKli8nt platform. Designed with simplicity and ease of use in mind, the UI provides<br>\na seamless experience for users to connect to their GitHub accounts, manage their<br>\nrepository specifications, and initiate deployments. We aim to streamline the<br>\napplication management process and empower developers to effortlessly deploy<br>\ntheir applications.</p>\n<p><img data-src=\"https://imgur.com/O5fW0GG.png\" alt=\"\"></p>\n<h2 id=\"user-authentication\"><a class=\"markdownIt-Anchor\" href=\"#user-authentication\">#</a> User Authentication</h2>\n<p>Kli8nt leverages the robust authentication capabilities of GitHub OAuth2 as its primary method for user login and authorization. By utilizing this powerful authentication mechanism, users can grant the platform access to their repositories. With the generated Personal Access Token (PAT) associated with the authenticated user, Kli8nt gains the ability to clone both private and public repositories, utilizing the functionalities provided by the GitHub API. This approach ensures secure and seamless integration with GitHub, allowing users to enjoy a streamlined experience while maintaining the necessary permissions and access controls.</p>\n<p><img data-src=\"https://imgur.com/KEoSXvr.png\" alt=\"\"></p>\n<h2 id=\"backend-service\"><a class=\"markdownIt-Anchor\" href=\"#backend-service\">#</a> Backend Service</h2>\n<p>The backend service is the backbone of the platform, ensuring efficient functionality by handling user requests, processing deployment tasks, managing logs, and facilitating seamless communication between components. It plays a vital role in request handling and routing, ensuring user requests are directed to the appropriate components for further processing. Additionally, it handles user authentication and authorization, ensuring secure access to the platform. Integration with RabbitMQ allows for the handling and queuing of deployment requests, while task scheduling ensures efficient and ordered processing. The backend service also integrates with GitHub’s Deployment Environment, facilitating synchronization and updates for deployments. Logs are streamed and stored using Kafka, with a backend endpoint responsible for streaming logs back to users for monitoring. Database interaction with PostgreSQL allows for data storage and retrieval related to user configurations, deployment records, and other relevant information, ensuring data consistency and integrity through GORM. Overall, the backend service acts as a crucial orchestrator, ensuring the smooth operation and effective management of the platform’s core functionalities.</p>\n<p><img data-src=\"https://i.imgur.com/WyYpyTw.png\" alt=\"\"></p>\n<h2 id=\"the-messaging-queue-system\"><a class=\"markdownIt-Anchor\" href=\"#the-messaging-queue-system\">#</a> The Messaging Queue System</h2>\n<p>To manage and queue deployment requests, we’ve integrated RabbitMQ. Simply put, it is a software where queues are defined, to which applications connect in order to transfer messages. For when a deployment request is received, the backend enqueues the task into a BUILD queue to the Kubernetes BUILD Operator. This ensures that each task is processed in the right order. The Kubernetes DEPLOY Operator in turn enqueus deployment results back here though the INFORM queue.</p>\n<p><img data-src=\"https://i.imgur.com/zNOA4Lz.png\" alt=\"\"></p>\n<h2 id=\"the-build-and-deploy-kubernetes-custom-controller\"><a class=\"markdownIt-Anchor\" href=\"#the-build-and-deploy-kubernetes-custom-controller\">#</a> The Build and Deploy Kubernetes Custom Controller</h2>\n<p>The clusters are responsible for the build and deployment operations. These processes are managed by custom controllers, ensuring efficiency and accuracy in deploying applications.</p>\n<p><img data-src=\"https://i.imgur.com/kTZmlxJ.png\" alt=\"\"></p>\n<h2 id=\"the-build-kubernetes-custom-controller\"><a class=\"markdownIt-Anchor\" href=\"#the-build-kubernetes-custom-controller\">#</a> The Build Kubernetes Custom Controller</h2>\n<p>The build controller plays a critical role in initiating the build process for each application. It acts as a listener for the queue served by RabbitMQ, enabling it to initiate the build process. When a new application is added to the queue, the controller creates an isolated environment specifically for that application. Within this isolated environment, it launches a dedicated build process. This process involves tasks such as cloning the repository and generating a Dockerfile based on the application’s stack and programming language, starting the Kaniko process to initiate the build, which includes compiling the source code, resolving dependencies, and creating a container image then pushing the resulting container image to a private registry.</p>\n<p>After completing the build and push process, the isolated environment is terminated and deleted, in addition, the controller publishes a message to the deployment queue served by RabbitMQ. This queue holds the applications that are scheduled to be deployed by the deploy controller. This ensures a clean and efficient workflow, as each build process operates in its isolated context. By leveraging the build controller’s capabilities, applications can be built and prepared for deployment without interference or conflicts with other ongoing builds</p>\n<h2 id=\"the-deploy-kubernetes-custom-controller\"><a class=\"markdownIt-Anchor\" href=\"#the-deploy-kubernetes-custom-controller\">#</a> The Deploy Kubernetes Custom Controller</h2>\n<p>After the build process concludes and the build controller sends an element to the queue, the deploy controller consumes the queue and initiates the deployment process. This involves creating a deployment object that includes a pod. The application’s Docker image is then pulled and run within this pod. Simultaneously, a cluster IP service is created and associated with the deployment, enabling internal communication within the cluster.</p>\n<p>The deploy controller takes charge of exposing the application externally and adding a TLS/SSL certificate. It achieves this by adding a route rule for the service within the ingress configuration. Additionally, it adds TLS/SSL configuration to the Ingress, allowing the application to be accessed securely via HTTPS. By utilizing Nginx Ingress, the controller ensures that the application is reachable and accessible with the desired HTTPS protocol and has a specific subdomain for the application.</p>\n<p>Once the application is successfully deployed and exposed to the TLS certificate, the deploy controller notifies the backend server that the application is now ready for user access. This communication informs the backend server that the newly deployed application is available and can be utilized by users.</p>\n<h2 id=\"the-log-streaming-controller\"><a class=\"markdownIt-Anchor\" href=\"#the-log-streaming-controller\">#</a> The Log Streaming Controller</h2>\n<p>Another important aspect in such provided service, is that the user would like to have a vision over his application while and after its being prepared and sent to production. To deliver such a feature, we have implemented another controller in go, that’s responsible for listening to Pod’s lifecycle events, made possible thanks to the Shared Informer feature in the Kubernetes API which is a local cache kind of resource made to be consumed. Whenever a new Deployment is up and running, grab its logs and stream them real time to another environment to be stored. This environment being Apache Kafka, an open source distributed message streaming that’s suited for high performance data pipelines. Our Kafka is running in its own managed cluster at Confluent, a KaaS provider with additional cool features that we liked while using them (like a nice user-friendly dashboard, a built-in REST API, etc…). Kafka is going to stream the logs, each application in its own kafka topic or event, directly to the backend, and store them for future requests -so you don’t really have to be there at the right time, whenever you request them you’ll get them and if you are there already you get them live.</p>\n<p><img data-src=\"https://i.imgur.com/UomqhLd.png\" alt=\"\"></p>\n<h1 id=\"the-conclusion\"><a class=\"markdownIt-Anchor\" href=\"#the-conclusion\">#</a> The conclusion</h1>\n<p>Our end-of-year project theoretically proposes and empirically validate a system design able to streamline the process of deploying any mciroservice application living in Github to Kubernetes on Cloud. This project is our gateway to our graduation projects as well as internship opportunities this summer, and would be a great addition to our professional journeys</p>\n<h1 id=\"meet-the-team\"><a class=\"markdownIt-Anchor\" href=\"#meet-the-team\">#</a> Meet the Team</h1>\n<p>Along with the immense support from our supervisor Mr. Bassem Ben Saleh and his esteemed jury.<br>\nWe would like to extend a heartfelt thank you to our families and close ones for their support and encouragement during late nights and long hours of work.</p>\n<div class=\"links\"><div class=\"item\" title=\"Mohamed Rafraf - Raf²\" style=\"--block-color:#2296fd;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9tb2hhbWVkLXJhZnJhZi5naXRodWIuaW8=\" data-background-image=\"https://media.licdn.com/dms/image/C4D03AQGoi9OLQy5yfg/profile-displayphoto-shrink_800_800/0/1635769397295?e=1691020800&v=beta&t=IOYj31PodnHo-vWlY3lHvfLTCAzBeGPclWqCkdQl1bE\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9tb2hhbWVkLXJhZnJhZi5naXRodWIuaW8=\">Mohamed Rafraf - Raf²</span>\n          <p class=\"desc\">Kubernetes Engineer</p>\n          </div></div><div class=\"item\" title=\"Adam Lahbib - Adm\" style=\"--block-color:#de2336;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9hZGFtbGFoYmliLm1l\" data-background-image=\"https://media.licdn.com/dms/image/D4D03AQEeg7KDib8qCQ/profile-displayphoto-shrink_800_800/0/1682525314941?e=1691020800&v=beta&t=O-T0VjepTffxrdxGNc4jwwPleeh-rt6RWlMF6gol1VQ\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9hZGFtbGFoYmliLm1l\">Adam Lahbib - Adm</span>\n          <p class=\"desc\">Backend Engineer / DevOps</p>\n          </div></div><div class=\"item\" title=\"Med Sofiene Barka - NieMand\" style=\"--block-color:#02bf1b;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly93d3cuaG1sbmRyLm1lLw==\" data-background-image=\"https://pbs.twimg.com/profile_images/1607462313182756867/WHEAVH9d_400x400.jpg\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly93d3cuaG1sbmRyLm1lLw==\">Med Sofiene Barka - NieMand</span>\n          <p class=\"desc\">Software Engineer</p>\n          </div></div><div class=\"item\" title=\"Skander Soltane - RedNax\" style=\"--block-color:#ff0378;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL3NrYW5kZXItc29sdGFuZQ==\" data-background-image=\"https://media.licdn.com/dms/image/D4D03AQGD3BI1PkShag/profile-displayphoto-shrink_800_800/0/1676590747987?e=1691020800&v=beta&t=Jo1SN1nqWZCCeJ7KyxP1f6NgWUPw4DyMGmkXPQrYvWg\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL3NrYW5kZXItc29sdGFuZQ==\">Skander Soltane - RedNax</span>\n          <p class=\"desc\">Software Developer</p>\n          </div></div><div class=\"item\" title=\"Dr. Bassem Ben Salah\" style=\"--block-color:#de2336;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL2Jhc3NlbS1iZW4tc2FsYWgtMzQ4N2EyNTkv\" data-background-image=\"https://media.licdn.com/dms/image/C4D03AQEuhp7TFHjE3Q/profile-displayphoto-shrink_800_800/0/1549533675903?e=1691020800&v=beta&t=AuBIN0Obuyu-dqcWr2HWZMS9qJNZKzAJiw5KnuVOLSg\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL2Jhc3NlbS1iZW4tc2FsYWgtMzQ4N2EyNTkv\">Dr. Bassem Ben Salah</span>\n          <p class=\"desc\">Our Supervisor</p>\n          </div></div></div>\n",
            "tags": []
        },
        {
            "id": "http://example.com/2023/04/28/Kubernetes/k8s-auth/",
            "url": "http://example.com/2023/04/28/Kubernetes/k8s-auth/",
            "title": "SSO-based authentication mechanism for multi-clusters",
            "date_published": "2023-04-28T10:29:37.000Z",
            "content_html": "<h1 id=\"introduction\"><a class=\"markdownIt-Anchor\" href=\"#introduction\">#</a> Introduction</h1>\n<h2 id=\"how-it-comes\"><a class=\"markdownIt-Anchor\" href=\"#how-it-comes\">#</a> How it comes</h2>\n<p>As a university student, I was assigned to create an authentication project for a specific system. But instead of choosing a traditional approach like Kerberos, I decided to spice things up and bring Kubernetes and cloud-native technologies into the mix (Typical Me). After diving into the official documentation, I was hit with a barrage of authentication strategies like X509 client certs, Static Token File, Bootstrap tokens, Service Accounts, OIDC tokens, and more. My head was spinning! Then I stumbled upon some existing authentication tools like Dex, Keycloak, and Pinniped. Terms like OIDC and OAuth2 were thrown around left and right. But I wasn’t intimidated! I accepted the challenge and declared: “It’s implementation time!” And thus, the k8s-auth project was born.</p>\n<h2 id=\"why-k8s-auth-is-born\"><a class=\"markdownIt-Anchor\" href=\"#why-k8s-auth-is-born\">#</a> Why k8s-auth is born</h2>\n<p>Managing access to Kubernetes clusters can be a challenging task, especially in large organizations where multiple teams and users need different levels of permissions. While Kubernetes provides a robust RBAC system, configuring and maintaining it can be time-consuming and error-prone, particularly if you have many clusters to manage.<br>\nIt can feel like you’re playing a never-ending game of whack-a-mole, trying to keep up with the constant changes and updates. And let’s be honest, it’s not the most exciting task in the world. Moving from one cluster to another, constantly changing contexts, generating kubeconfig for users, and dealing with permission problems can quickly become a chore.</p>\n<p><img data-src=\"https://imgur.com/a5zaVhH.png\" alt=\"\"></p>\n<h2 id=\"before-starting\"><a class=\"markdownIt-Anchor\" href=\"#before-starting\">#</a> Before Starting</h2>\n<p>Before you start reading this blog post, let’s make sure you’re ready to dive into the k8s-auth server world. First, make sure you have a solid understanding of Kubernetes basics and RBAC, as we’ll be building on those concepts. Don’t worry, we won’t ask you to explain any Kubernetes concept in binary code. We just need you to understand what a service account is, what roles and role bindings mean, and why a service account needs a secret. So, if these concepts are still a mystery to you, go ahead and brush up on your Kubernetes knowledge before jumping in. And if you need a refresher, don’t worry, we won’t judge you. We’ll just send you back to Kubernetes school.</p>\n<h1 id=\"k8s-auth-the-sso-based-authentication-mechanism-for-multi-clusters\"><a class=\"markdownIt-Anchor\" href=\"#k8s-auth-the-sso-based-authentication-mechanism-for-multi-clusters\">#</a> K8S-AUTH : The SSO-Based Authentication Mechanism for Multi-Clusters!</h1>\n<h2 id=\"what-is-k8s-auth\"><a class=\"markdownIt-Anchor\" href=\"#what-is-k8s-auth\">#</a> What is k8s-auth ?</h2>\n<p>K8s-auth is a tool that provides authentication and authorization mechanisms for multiple Kubernetes clusters. With k8s-auth, users can authenticate to multiple clusters using a single set of credentials. This eliminates the need for users to have different credentials for different clusters, simplifying the authentication process. K8s-auth also allows administrators to manage user and group permissions across multiple clusters from a single centralized location, making it easier to maintain security and access control.</p>\n<p><img data-src=\"https://imgur.com/awkZvAb.png\" alt=\"\"></p>\n<h2 id=\"the-authentication-server\"><a class=\"markdownIt-Anchor\" href=\"#the-authentication-server\">#</a> The Authentication server</h2>\n<p>“K8s-Auth Server” is an open-source authentication server designed to simplify the authorization and authentication process for users and groups within specific Kubernetes clusters. With this server, administrators can easily manage access controls for multiple clusters, all from a centralized location.</p>\n<p>With the k8s-auth server, you can say goodbye to the headache of managing RBAC configurations across multiple clusters. Instead, you can focus on the fun part of being an admin - creating users, deleting users, modifying users, creating groups, deleting groups, modifying groups, and more! And the best part? You get to do it all with a single set of credentials, like a master key that unlocks all the doors to the kingdom of Kubernetes.</p>\n<p>But wait, there’s more! With SSO-based access, users can connect to multiple clusters with different permissions based on their role on each cluster, all without having to enter a password a million times. It’s like having a backstage pass to all the coolest Kubernetes events without ever having to wait in line.</p>\n<h2 id=\"how-this-server-communicate-with-clusters\"><a class=\"markdownIt-Anchor\" href=\"#how-this-server-communicate-with-clusters\">#</a> How This Server Communicate with Clusters ?</h2>\n<p>K8s-Auth Server is designed to work seamlessly with K8s-Auth Controller, a cloud-native application that is deployed within the Kubernetes cluster. This controller acts as an agent for the server, facilitating communication between the server and the cluster.</p>\n<p><img data-src=\"https://imgur.com/0PK9Sj5.png\" alt=\"\"></p>\n<p>Before a cluster can be used with K8s-Auth Server, it must be registered with the server by the administrator. When a cluster is registered, a secret token is created, which must be used by the agent/controller deployed within the cluster to verify its existence. Once the agent/controller is verified, a connection between the cluster and the server is opened (web socket) for data communication.</p>\n<p><img data-src=\"https://imgur.com/F1EVKK3.png\" alt=\"\"></p>\n<h2 id=\"how-to-interact-with-this-server\"><a class=\"markdownIt-Anchor\" href=\"#how-to-interact-with-this-server\">#</a> How to interact with this Server</h2>\n<p>K8s-Auth Server comes with a powerful CLI utility that can be used by both administrators and users. The CLI behaves differently based on the user’s role - administrators can create, modify, and delete users, groups, and clusters, while normal users can check the clusters that they are authorized to connect with. Once the users selects the cluster they want to authenticate with, the CLI will automatically generate a kubeconfig file, making it easy for users to connect to the cluster and start working.</p>\n<h2 id=\"how-users-logs-to-the-server\"><a class=\"markdownIt-Anchor\" href=\"#how-users-logs-to-the-server\">#</a> How users logs to the server</h2>\n<p>To connect to any cluster, users must first authenticate with the K8s-Auth Server. Authentication with the server is done using OAuth2 with Google, allowing users to use one set of credentials to connect to multiple clusters with different permissions based on their role on each cluster. This type of authentication can be referred to as SSO-based authentication.</p>\n<h2 id=\"what-happen-when-user-want-to-authenticate-to-a-cluster\"><a class=\"markdownIt-Anchor\" href=\"#what-happen-when-user-want-to-authenticate-to-a-cluster\">#</a> What happen when user want to authenticate to a cluster ?</h2>\n<p>On the user side, when a user logs into the K8s-Auth Server, they can check the clusters they are authorized to access. After selecting the desired cluster, a request is sent to the server, which checks if the user is authorized to connect. If authorized, the server sends a request to the specific agent/controller, which creates the necessary Kubernetes resources, such as service accounts, roles, role bindings, and secrets, allowing the user to connect to the cluster and return the token and ca.crt that allow the automatic generation of the kubeconfig.</p>\n<h2 id=\"the-agentcontroller-role\"><a class=\"markdownIt-Anchor\" href=\"#the-agentcontroller-role\">#</a> The Agent/Controller role</h2>\n<p>The agent/controller also manages the lifecycle of the secret and token, deleting them and all related roles and bindings after they expire or are no longer needed. For example, if an administrator decides to remove a user from a cluster, the agent/controller will delete the secret that holds the user’s token, as well as the service account, roles, and bindings associated with that user.</p>\n<h2 id=\"before-demo\"><a class=\"markdownIt-Anchor\" href=\"#before-demo\">#</a> Before DEMO</h2>\n<div class=\"note danger\">\n<p>Please note that this project is open source and the code is available to anyone interested. It is currently an MVP version that functions effectively, and we plan to add more features in the future.</p>\n</div>\n<div class=\"note warning\">\n<p>Don’t forget that this project is open to contributions from the community! If you find a bug or have a feature request, feel free to open an issue on the project’s GitHub page. And if you’re interested in helping out with the development, pull requests are always welcome! Let’s work together to make this project even better.</p>\n</div>\n<div class=\"links\"><div class=\"item\" title=\"k8s-auth-server\" style=\"--block-color:#2296fd;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLXNlcnZlcg==\" data-background-image=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLXNlcnZlcg==\">k8s-auth-server</span>\n          <p class=\"desc\">The authentication server</p>\n          </div></div><div class=\"item\" title=\"k8s-auth-kube\" style=\"--block-color:#de2336;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLWt1YmU=\" data-background-image=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLWt1YmU=\">k8s-auth-kube</span>\n          <p class=\"desc\">The Controller/Agent for k8s-auth</p>\n          </div></div><div class=\"item\" title=\"k8s-auth-cli\" style=\"--block-color:#02bf1b;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLWNsaQ==\" data-background-image=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01vaGFtZWQtUmFmcmFmL2s4cy1hdXRoLWNsaQ==\">k8s-auth-cli</span>\n          <p class=\"desc\">The CLI utility for k8s-auth</p>\n          </div></div></div>\n<h1 id=\"the-demo\"><a class=\"markdownIt-Anchor\" href=\"#the-demo\">#</a> The DEMO !</h1>\n<p>And the icing on the cake? I am going to show you how to set up and use k8s-auth in a demo that will blow your mind (in a good way, of course). So grab your popcorn and let’s get this Kubernetes party started!</p>\n<p>Ladies and Gentlemen, it’s showtime! Let’s get ready to k8s-authenticate! We’ll be putting the spotlight on three stars of this demo: The Authentication Server, The Agent/Controller, and the Command Line Utility! These babies were born to shine in the Go language (talk about power!) and are all open-source, so you can check them out on my GitHub page. And, to make things even easier, we’ve containerized the authentication server and agent, so you can just pull them like a cold one on a hot summer day. Check out my Docker Hub profile for more juicy details!</p>\n<h2 id=\"setting-up-authentication-server\"><a class=\"markdownIt-Anchor\" href=\"#setting-up-authentication-server\">#</a> Setting up Authentication Server</h2>\n<p>Hold your horses, folks! The authentication server may be containerized, but it’s not going inside Kubernetes! Don’t forget to set up those environment variables for the server - especially the admin email! (He deserve a name and a full name too, don’t they?) And guess what, the admin can log in with Google OAuth2! Just make sure to give the Client ID, Client Secret, and Redirection Link, or else things won’t work too smoothly.</p>\n<blockquote>\n<p>You need to create an OAuth 2.0 Client with the Google API in google cloud platform!</p>\n</blockquote>\n<p><img data-src=\"https://imgur.com/YqhD79J.png\" alt=\"\"></p>\n<ul>\n<li><code>K8S_AUTH_ADMIN_NAME</code> : The administrator name! (My name is default value)</li>\n<li><code>K8S_AUTH_ADMIN_FULLNAME</code> : The administrator name! (My name is default value)</li>\n<li><code>K8S_AUTH_ADMIN_MAIL</code> : The administrator name! (My email is default value)</li>\n<li><code>OAUTH2_CLIENT_ID</code> : The google Oauth2 Client ID (Required)</li>\n<li><code>OAUTH2_CLIENT_SECRET</code> : The google Oauth2 Secret  (Required)</li>\n<li><code>OAUTH2_REDIRECT_URL</code> : The google Oauth2 Redirect URL callback (Required, it must be redirected to /callback endpoint of your server )</li>\n</ul>\n<blockquote>\n<p>Let’s assume that the server is deployed on <span class=\"exturl\" data-url=\"aHR0cHM6Ly9hdXRoLjRuNm5rOHMudGVjaA==\">https://auth.4n6nk8s.tech</span>,<br>\n <code>OAUTH2_REDIRECT_URL</code>  must be <span class=\"exturl\" data-url=\"aHR0cHM6Ly9hdXRoLjRuNm5rOHMudGVjaC9jYWxsYmFjaw==\">https://auth.4n6nk8s.tech/callback</span></p>\n</blockquote>\n<p>It’s docker time!  <code>mohamedrafraf/k8s-auth-server</code>  is the docker image that you need! Let’s run this server!</p>\n<figure class=\"highlight bash\"><figcaption><span>run server</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --name server -it -p 80:8080 -e OAUTH2_CLIENT_ID=xxxx -e OAUTH2_CLIENT_SECRET=xxx ... mohamedrafraf/k8s-auth-server</span><br></pre></td></tr></table></figure>\n<p>Make sure to input all the necessary variables for the server to run smoothly. And once you’ve done that, sit back and relax, and let the server do its thing! Keep your hands off those logs, let them flow in peace!</p>\n<h2 id=\"interacting-with-server-using-k8s-auth-cli\"><a class=\"markdownIt-Anchor\" href=\"#interacting-with-server-using-k8s-auth-cli\">#</a> Interacting with server using k8s-auth CLI</h2>\n<p>It’s CLI installation time, folks! This nifty command line utility is your ticket to the k8s-auth party. With the CLI, you can log in as an admin or regular user and interact with the server based on your permission level.</p>\n<p>But wait, there’s more! For the first time, only the admin with their fancy schmancy email address can access the server. They’ll be the ones registering the cluster, setting up the agent, and creating a connection between the server and the agent. Then, it’s time to have some fun with users, groups, and permissions. Who said authentication couldn’t be funny?</p>\n<p>You can build the command line utility or download it by running:</p>\n<figure class=\"highlight bash\"><figcaption><span>download cli</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://github.com/Mohamed-Rafraf/k8s-auth-cli/releases/download/test/k8s-auth</span><br><span class=\"line\">sudo <span class=\"built_in\">chmod</span> +x k8s-auth &amp;&amp; sudo <span class=\"built_in\">mv</span> k8s-auth /usr/bin</span><br></pre></td></tr></table></figure>\n<p>Now, you can use this command line utility just like any other tool in your system!</p>\n<p><img data-src=\"https://imgur.com/EciPajC.png\" alt=\"\"></p>\n<p>Before start playing with this command line you need to know that on each command you need to specify the authentication server that you want to interact with! So either you user  <code>--server</code>  on each command or use  <code>K8S_AUTH_SERVER</code>  environment variable to make it more easy for you! I’ll go with the 2nd option!</p>\n<figure class=\"highlight bash\"><figcaption><span>export env var</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> K8S_AUTH_SEVER=https://auth.4n6nk8s.tech</span><br></pre></td></tr></table></figure>\n<p>Now you need to log in as an adminstrator! It’s not that hard just run  <code>k8s-auth login</code>  and forget to specify  <code>--admin</code>  to login as the super user!</p>\n<p><img data-src=\"https://imgur.com/10prjzn.png\" alt=\"\"></p>\n<p>The utility needs a secret token, which you’ll get from an URL link. This link will take you to Google for authentication and authorization as an admin. Once you’re authorized, you’ll get the secret token to paste and everything will be good to go.</p>\n<p>You’ll get something like this one in case you’re authorized!<br>\n<img data-src=\"https://imgur.com/Yt9FZgD.png\" alt=\"\"></p>\n<p>Copy that thing don’t worry! Once you paste it, the utility will verify the token and let you know if you’re ready for your actions or not!</p>\n<p><img data-src=\"https://imgur.com/MY7TTOZ.png\" alt=\"\"></p>\n<p>Bingo we are ready for action! Now as an admin you can register clusters, create,delete,modify users and groups (and their permissions) inside each cluster!</p>\n<blockquote>\n<p>Let’s assume that you have 2 clusters! You create a user inside cluster1 and you give him permission to list pods on dev namespace. You can add that user to cluster2 with other permission like create secrets on prod namespace</p>\n</blockquote>\n<p>It’s time to register a cluster! You’ll recieve a secret token.</p>\n<p><img data-src=\"https://imgur.com/yvANfkB.png\" alt=\"\"></p>\n<p>And as you see here! The cluster is registered using  <code>k8s-auth create cluster</code>  and you can list registred clusters with  <code>k8s-auth get clusters</code></p>\n<blockquote>\n<p>Keep in mind that registerd cluster have 2 status, not Active when the cluster is not verified yet. The Active status will show you the API SERVER address/hostname</p>\n</blockquote>\n<blockquote>\n<p>Only Admins can see the tokens! Running the same command as a regular user will list you only the clusters that you have access to it (so not all cluster) and without the token!</p>\n</blockquote>\n<p>Let’s take rest now from the CLI. It’s time for kubernetes! Let’s deploy the agent/controller that will communicate with the server!</p>\n<h2 id=\"deploy-the-agentcontroller-inside-kubernetes\"><a class=\"markdownIt-Anchor\" href=\"#deploy-the-agentcontroller-inside-kubernetes\">#</a> Deploy the Agent/Controller inside Kubernetes</h2>\n<p>Deploying the agent is not rocket science! Deploying the agent is not rocket science! (To be honest i didn’t make a helm chart yet, Sadly 😦 ). But if you want to do it the old fashioned way, no worries! It’s still a piece of cake. Just keep in mind that you need to create a namespace called “k8s-auth” in your cluster. This namespace will contain all the service accounts and secrets of the users that authenticate and have a session with the cluster.</p>\n<p>The agent’s mission is to make sure everyone gets what they need! It creates service accounts, roles, and role bindings to make sure users have the right permissions in the cluster. That’s why the agent itself needs a service account and permissions to do its job inside the cluster.</p>\n<p>Let’s create the namespace and the service account for this agent!</p>\n<figure class=\"highlight yaml\"><figcaption><span>create ns & sa</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Namespace</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span> &#123;&#125;</span><br><span class=\"line\"><span class=\"attr\">status:</span> &#123;&#125;</span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">ServiceAccount</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">k8s-auth</span></span><br></pre></td></tr></table></figure>\n<p>This service account need a clusterrole that allow to create roles to anything!  to grant permission for creating pods you must have this permission first! You can’t give permission of create something and you can’t do it!!!</p>\n<p>Next, the service account will need an appropriate permissions to carry out its mission inside the cluster. Specifically, it requires a clusterrole that grants permission to create roles for any resource.</p>\n<p>In kubernetes world, granting permission to create something requires that you have the permission yourself!. For example, you can’t grant permission to create pods if you don’t have permission to create pods even you have permission to create roles!!</p>\n<p>Let’s create the clusterole and the binding!</p>\n<figure class=\"highlight yaml\"><figcaption><span>create cluster role</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">rbac.authorization.k8s.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">ClusterRole</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\"><span class=\"attr\">rules:</span></span><br><span class=\"line\"><span class=\"bullet\">-</span> <span class=\"attr\">apiGroups:</span> [<span class=\"string\">&quot;*&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">resources:</span> [<span class=\"string\">&quot;*&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">verbs:</span> [<span class=\"string\">&quot;*&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">rbac.authorization.k8s.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">ClusterRoleBinding</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\"><span class=\"attr\">roleRef:</span></span><br><span class=\"line\">  <span class=\"attr\">apiGroup:</span> <span class=\"string\">rbac.authorization.k8s.io</span></span><br><span class=\"line\">  <span class=\"attr\">kind:</span> <span class=\"string\">ClusterRole</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\"><span class=\"attr\">subjects:</span></span><br><span class=\"line\"><span class=\"bullet\">-</span> <span class=\"attr\">kind:</span> <span class=\"string\">ServiceAccount</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">k8s-auth</span></span><br></pre></td></tr></table></figure>\n<p>Now it’s time to the last part! we will deploy the agent itself! The agent need to know some information so as expected there is some environement variable!</p>\n<ul>\n<li><code>CLUSTER_NAME</code> : is the same name that you put it inside the authentication server</li>\n<li><code>TOKEN</code> : The generated token from the authentication server</li>\n<li><code>SERVER</code> : The authentication server itself</li>\n<li><code>API_SERVER</code> : This is contains the public hostname for the API SERVER. The agent can detect it correctly when you have a cluster inside Network and the cluster is not exposed in the internet</li>\n</ul>\n<blockquote>\n<p>I used kubeadm clusters in provisionning clusters. I didn’t find a way to find the public hostname! So you need to indicate the api server until it will be fixed!</p>\n</blockquote>\n<figure class=\"highlight yaml\"><figcaption><span>deploy</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">labels:</span></span><br><span class=\"line\">    <span class=\"attr\">app:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">replicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">matchLabels:</span></span><br><span class=\"line\">      <span class=\"attr\">app:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">  <span class=\"attr\">strategy:</span> &#123;&#125;</span><br><span class=\"line\">  <span class=\"attr\">template:</span></span><br><span class=\"line\">    <span class=\"attr\">metadata:</span></span><br><span class=\"line\">      <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">      <span class=\"attr\">labels:</span></span><br><span class=\"line\">        <span class=\"attr\">app:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">    <span class=\"attr\">spec:</span></span><br><span class=\"line\">      <span class=\"attr\">serviceAccountName:</span> <span class=\"string\">k8s-auth</span></span><br><span class=\"line\">      <span class=\"attr\">containers:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">image:</span> <span class=\"string\">mohamedrafraf/k8s-auth-kube</span> </span><br><span class=\"line\">        <span class=\"attr\">name:</span> <span class=\"string\">k8s-auth-kube</span></span><br><span class=\"line\">        <span class=\"attr\">env:</span></span><br><span class=\"line\">          <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">CLUSTER_NAME</span></span><br><span class=\"line\">            <span class=\"attr\">value:</span> <span class=\"string\">ctf-cluster</span></span><br><span class=\"line\">          <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">TOKEN</span></span><br><span class=\"line\">            <span class=\"attr\">value:</span> <span class=\"string\">&quot;Z8taw1yFw4lsq7cgSvmZ&quot;</span></span><br><span class=\"line\">          <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">SERVER</span></span><br><span class=\"line\">            <span class=\"attr\">value:</span> <span class=\"string\">&quot;https://auth.4n6nk8s.tech&quot;</span></span><br><span class=\"line\">          <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">API_SERVER</span></span><br><span class=\"line\">            <span class=\"attr\">value:</span> <span class=\"string\">https://172.190.91.84:6443</span></span><br><span class=\"line\">        <span class=\"attr\">resources:</span> &#123;&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"attr\">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>After deploying the agent. you can check the clusters with the cli again and you’ll find everything is ok!<br>\n<img data-src=\"https://imgur.com/MqHeeY1.png\" alt=\"\"></p>\n<p>The cluster is on Active status and the api server is not empty now! You can repeat this operation with many clusters and this is how multi-clusters can work together!</p>\n<h2 id=\"create-users-and-groups\"><a class=\"markdownIt-Anchor\" href=\"#create-users-and-groups\">#</a> Create Users and Groups</h2>\n<p>The cluster is now ready for authentication and has been verified to be functioning without any issues. The connection between the cluster and authentication has been established. Now, let’s proceed to creating groups and users.</p>\n<p>Let’s see what  <code>k8s-auth create</code>  can do for us!</p>\n<p><img data-src=\"2023-04-29-15-56-38.png\" alt=\"\"></p>\n<p>Great news! With the  <code>k8s-auth create</code>  command, you can easily create groups and users for your authenticated clusters. To create a group, simply specify the cluster and provide a YAML manifest file containing the roles for that group. For creating a user, you can either assign them to an existing group or provide a YAML manifest file for the user’s roles if they do not belong to any group.</p>\n<p>Let’s define the role for our group that we will create!</p>\n<figure class=\"highlight yaml\"><figcaption><span>group rule</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Role</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">rbac.authorization.k8s.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">dev</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">pod-and-secret</span></span><br><span class=\"line\"><span class=\"attr\">rules:</span></span><br><span class=\"line\"><span class=\"bullet\">-</span> <span class=\"attr\">apiGroups:</span> [<span class=\"string\">&quot;&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">resources:</span> [<span class=\"string\">&quot;pods&quot;</span>,<span class=\"string\">&quot;secrets&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">verbs:</span> [<span class=\"string\">&quot;list&quot;</span>,<span class=\"string\">&quot;create&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Role</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">rbac.authorization.k8s.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">prod</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">pod-list</span></span><br><span class=\"line\"><span class=\"attr\">rules:</span></span><br><span class=\"line\"><span class=\"bullet\">-</span> <span class=\"attr\">apiGroups:</span> [<span class=\"string\">&quot;&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">resources:</span> [<span class=\"string\">&quot;pods&quot;</span>]</span><br><span class=\"line\">  <span class=\"attr\">verbs:</span> [<span class=\"string\">&quot;list&quot;</span>]</span><br></pre></td></tr></table></figure>\n<p>The group we’re about to create will be granted permission to create and list pods and secrets in the “dev” namespace, while they will only be able to list pods in the “prod” namespace.</p>\n<blockquote>\n<p>Any user will be belong to this group will have these permissions! You don’t have to repeat the same manifest for each user!. This is why groups exists!</p>\n</blockquote>\n<p>This command will create the group for you</p>\n<figure class=\"highlight bash\"><figcaption><span>command</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k8s-auth create group &lt;name&gt; --cluster=&lt;cluster&gt; --file=&lt;rbac_file&gt;</span><br></pre></td></tr></table></figure>\n<p><img data-src=\"https://imgur.com/g5YBB8u.png\" alt=\"\"></p>\n<p>As you can see here! The group is create successfully and you can see his permission when you forget it using  <code>k8s-auth get permission</code>  command!</p>\n<p>You can discover what you can do with  <code>k8s-auth get</code>  command!! You can a list users too!</p>\n<p><img data-src=\"https://imgur.com/MOC7L1o.png\" alt=\"\"></p>\n<p>Now let’s create a user that belong to this group! So we don’t need to specify any manifest file that define roles!</p>\n<figure class=\"highlight bash\"><figcaption><span>user</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k8s-auth create user --name &lt;name&gt; --fullname &lt;value&gt; --mail &lt;mail&gt; --cluster &lt;cluster&gt; --group &lt;group&gt;</span><br></pre></td></tr></table></figure>\n<p>In case that you want to create a user that don’t belongs to any group and have his own permission you can delete the  <code>--group</code>  and change it with  <code>--file</code>  and specify the yaml file that contains his roles!</p>\n<blockquote>\n<p>No need to worry about naming roles because there won’t be any overlapping between them. The agent takes care of creating roles and bindings in the background, and the names you specify in the manifest won’t be used directly.</p>\n</blockquote>\n<p><img data-src=\"https://imgur.com/n2S8BQM.png\" alt=\"\"></p>\n<p>You can update the permission using the  <code>k8s-auth update permisssion</code>  command! This command can update groups and users permission</p>\n<p>In case you change the permission for a user that belgons to a group. This user will leave that group because he will have his own permissions (different from the group one)</p>\n<blockquote>\n<p>You can’t delete a group if is not empty!</p>\n</blockquote>\n<blockquote>\n<p>When User have an opened session with a cluster and use kubectl normally and you decide to delete that user the session will be closed and he can’t do anything!</p>\n</blockquote>\n<h2 id=\"authenticate-to-a-cluster\"><a class=\"markdownIt-Anchor\" href=\"#authenticate-to-a-cluster\">#</a> Authenticate to a cluster</h2>\n<p>As a regular user you need to login to the authentication server using  <code>k8s-auth login</code>  and you’ll receive a link as the admin login process!</p>\n<p>Then you can list the clusters that you can authenticate with using  <code>k8s-auth get clusters</code>  and keep in your mind that you’ll never see the token of each cluster (Only admins can see tokens)</p>\n<p>Once you decided which cluster you’ll authenticate! run the command  <code>k8s-auth auth</code>  with specifying the cluster name!</p>\n<figure class=\"highlight bash\"><figcaption><span>authenticate</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">k8s-auth auth cluster=ctf-cluster</span><br><span class=\"line\"></span><br><span class=\"line\">To use this cluster run this <span class=\"built_in\">command</span>: <span class=\"built_in\">export</span> KUBECONFIG=<span class=\"variable\">$HOME</span>/.k8s-auth.config</span><br></pre></td></tr></table></figure>\n<p>If you’re authorized, you’ll receive a message like that! What is happening?</p>\n<p>Actually the k8s-auth CLI generate a kubeconfig file for you! run the command that the CLI suggest to you! BOOOM Start Kubectlying !!! xD</p>\n<h2 id=\"conclusion\"><a class=\"markdownIt-Anchor\" href=\"#conclusion\">#</a> Conclusion</h2>\n<p>As highlighted in the blog post, the process of authentication has been simplified with just 3 simple commands - login, auth, and export KUBECONFIG. These commands enable users to easily open a session with any cluster they want, regardless of the number of clusters and different permissions required, using just a single set of credentials. Additionally, the administrator can easily set up agents in the clusters, and manage the permissions and the entire process centrally using a smooth command line interface.</p>\n",
            "tags": []
        },
        {
            "id": "http://example.com/2022/09/10/Kubernetes/nfs-k8s/",
            "url": "http://example.com/2022/09/10/Kubernetes/nfs-k8s/",
            "title": "NFS as Remote Storage for Kubernetes",
            "date_published": "2022-09-10T22:26:58.000Z",
            "content_html": "<p>We will setting up a NFS server to use it as remote storage for our cluster to create a lot of persistent volumes in our local infrastructure !</p>\n<p>We Assume that we have 4 Ubuntu 20.04 LTS, The Kubernetes is installed and the  <code>4n6nk8s-nfs</code>  host in the same network with the cluster :</p>\n<table>\n<thead>\n<tr>\n<th>Role</th>\n<th>Hostname</th>\n<th>IP address</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Master</td>\n<td>4n6nk8s-master</td>\n<td>192.168.1.18/24</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>4n6nk8s-worker1</td>\n<td>192.168.1.19/24</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>4n6nk8s-worker2</td>\n<td>192.168.1.20/24</td>\n</tr>\n<tr>\n<td>NFS Server</td>\n<td>4n6nk8s-nfs</td>\n<td>192.168.1.80/24</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"what-is-a-nfs-network-file-system-server\"><a class=\"markdownIt-Anchor\" href=\"#what-is-a-nfs-network-file-system-server\">#</a> What is a NFS (Network File System) Server:</h2>\n<p>Network File System (NFS) is a networking protocol for distributed file sharing. A file system defines the way data in the form of files is stored and retrieved from storage devices, such as hard disk drives, solid-state drives and tape drives. NFS is a network file sharing protocol that defines the way files are stored and retrieved from storage devices across networks.</p>\n<p>This distributed file system protocol allows a user on a client computer to access files over a network in the same way they would access a local storage file.</p>\n<h2 id=\"setting-up-the-nfs-server\"><a class=\"markdownIt-Anchor\" href=\"#setting-up-the-nfs-server\">#</a> Setting up the NFS server</h2>\n<p>We need to install the  <code>nfs-kernel-server</code>  package on the NFS server. This package will store additional packages such as  <code>nfs-common</code>  and  <code>4n6nk8s@rpcbind</code></p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s-nfs:~$ sudo apt install nfs-kernel-server</span><br></pre></td></tr></table></figure>\n<p>Now let’s create an NFS Export Directory</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo <span class=\"built_in\">mkdir</span> /mnt/nfs-data </span><br></pre></td></tr></table></figure>\n<p>Now let’s give it a read,write and execute privileges to all the contents inside the directory</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo <span class=\"built_in\">chmod</span> 777 /mnt/nfs-data</span><br></pre></td></tr></table></figure>\n<p>Now Lets add a new line to the  <code>/etc/exports</code>  configuration file.</p>\n<blockquote>\n<p>The  <code>/etc/exports</code>  file indicates all directories that a nfs server exports to its clients. Each line in the file specifies a single directory.</p>\n</blockquote>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo vim /etc/exports</span><br></pre></td></tr></table></figure>\n<p>You can provide access to a single client, multiple clients, or specify an entire subnet. In this guide, we have allowed an entire subnet to have access to the NFS share.</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/mnt/nfs-data <span class=\"number\">192.168</span>.<span class=\"number\">1.0</span>/<span class=\"number\">24</span>(rw,<span class=\"keyword\">sync</span>,no_subtree_check)</span><br></pre></td></tr></table></figure>\n<p>After granting access to the subnet, let’s export the NFS share directory and restart the NFS</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo exportfs -a</span><br><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo systemctl restart nfs-kernel-server</span><br></pre></td></tr></table></figure>\n<p>Let’s allow NFS access through the firewall</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ sudo ufw allow from 192.168.43.0/24 to any port nfs</span><br></pre></td></tr></table></figure>\n<h2 id=\"install-the-nfs-client-on-the-kubernetes-nodes\"><a class=\"markdownIt-Anchor\" href=\"#install-the-nfs-client-on-the-kubernetes-nodes\">#</a> Install the NFS Client on the Kubernetes Nodes</h2>\n<p>We must install the  <code>nfs-common</code>  packages to access to the NFS share so let’s install it by running the following command on each node:</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ sudo apt install nfs-common</span><br></pre></td></tr></table></figure>\n<p>This command mount the NFS Share on one node for testing and sanity check only</p>\n<blockquote>\n<p>The mount command is not a mandatory step. We mount for testing purposes. you can skip to the next section</p>\n</blockquote>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ sudo mount 4n6nk8s-nfs:/mnt/nfs-data  /mnt</span><br></pre></td></tr></table></figure>\n<p>Let’s Create a file for testing</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ <span class=\"built_in\">cd</span> /mnt</span><br><span class=\"line\">4n6nk8s@4n6nk8s-worker1:/mnt $ <span class=\"built_in\">touch</span> file </span><br></pre></td></tr></table></figure>\n<p>Check the  <code>/mnt/nfs-data</code>  on the NFS server</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-nfs:~$ <span class=\"built_in\">cd</span> /mnt/nfs-data</span><br><span class=\"line\">4n6nk8s@4n6nk8s-nfs:/mnt/nfs-data$ <span class=\"built_in\">ls</span></span><br><span class=\"line\">file </span><br></pre></td></tr></table></figure>\n<h2 id=\"kubernetes-with-nfs-remote-storage-demo\"><a class=\"markdownIt-Anchor\" href=\"#kubernetes-with-nfs-remote-storage-demo\">#</a> Kubernetes with NFS remote Storage demo</h2>\n<p>After Setting up the NFS server and install the NFS client on the kubernetes nodes. Now it’s time to do some practice with  <code>Persistent Volume</code>  and  <code>Persistent Volume Claim</code>  with NFS storage.</p>\n<h3 id=\"create-a-persistent-volume-with-nfs\"><a class=\"markdownIt-Anchor\" href=\"#create-a-persistent-volume-with-nfs\">#</a> Create a Persistent Volume with NFS</h3>\n<p>Example of Persistent Volume manifest using nfs:</p>\n<figure class=\"highlight yaml\"><figcaption><span>PersistentVolume</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">PersistentVolume</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">pv-nfs</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">capacity:</span></span><br><span class=\"line\">    <span class=\"attr\">storage:</span> <span class=\"string\">100Mi</span></span><br><span class=\"line\">  <span class=\"attr\">accessModes:</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"string\">ReadWriteMany</span></span><br><span class=\"line\">  <span class=\"attr\">storageClassName:</span> <span class=\"string\">nfs</span></span><br><span class=\"line\">  <span class=\"attr\">nfs:</span></span><br><span class=\"line\">    <span class=\"attr\">server:</span> <span class=\"number\">192.168</span><span class=\"number\">.1</span><span class=\"number\">.80</span> <span class=\"comment\"># the IP address of 4n6nk8s-nfs host</span></span><br><span class=\"line\">    <span class=\"attr\">path:</span> <span class=\"string\">&quot;/mnt/nfs-data&quot;</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Make sure to put the correct IP address of the NFS server and the correct NFS Share point!<br>\nCreate the persistent volume using kubectl</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl apply -f nfs-pv.yaml</span><br><span class=\"line\">persistentvolume/pv-nfs created</span><br></pre></td></tr></table></figure>\n<p>List the Persistent Volumes to make sure for the creation</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl get pv</span><br><span class=\"line\">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class=\"line\">pv-nfs   100Mi      RWX            Retain           Available           nfs                     4s</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"create-a-persistent-volume-claim-with-nfs\"><a class=\"markdownIt-Anchor\" href=\"#create-a-persistent-volume-claim-with-nfs\">#</a> Create a Persistent Volume Claim with NFS</h3>\n<p>Example of Persistent Volume Claim manifest using nfs:</p>\n<figure class=\"highlight yaml\"><figcaption><span>PersistentVolumeClaim</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">PersistentVolumeClaim</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">pvc-nfs</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">accessModes:</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"string\">ReadWriteMany</span></span><br><span class=\"line\">  <span class=\"attr\">storageClassName:</span> <span class=\"string\">nfs</span></span><br><span class=\"line\">  <span class=\"attr\">resources:</span></span><br><span class=\"line\">    <span class=\"attr\">requests:</span></span><br><span class=\"line\">        <span class=\"attr\">storage:</span> <span class=\"string\">100Mi</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Create the persistent volume using kubectl</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl apply -f nfs-pvc.yaml</span><br><span class=\"line\">persistentvolumeclaim/pvc-nfs created</span><br></pre></td></tr></table></figure>\n<p>List the  <code>Persistent Volumes Claims</code>  to make sure for the creation</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl get pvc</span><br><span class=\"line\">NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class=\"line\">pvc-nfs   Bound    pv-nfs   100Mi      RWX            nfs            3s</span><br></pre></td></tr></table></figure>\n<h3 id=\"create-nginx-deployment\"><a class=\"markdownIt-Anchor\" href=\"#create-nginx-deployment\">#</a> Create Nginx Deployment</h3>\n<p>We use the  <code>volumeMounts</code>  and  <code>volumes </code> attributes in this manifest to use the persistent volume we created:</p>\n<figure class=\"highlight yaml\"><figcaption><span>Deployment</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">    <span class=\"attr\">name:</span> <span class=\"string\">nginx-deployment</span></span><br><span class=\"line\">    <span class=\"attr\">labels:</span></span><br><span class=\"line\">        <span class=\"attr\">role:</span> <span class=\"string\">webserver</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">    <span class=\"attr\">replicas:</span> <span class=\"number\">3</span></span><br><span class=\"line\">    <span class=\"attr\">selector:</span></span><br><span class=\"line\">        <span class=\"attr\">matchLabels:</span></span><br><span class=\"line\">            <span class=\"attr\">role:</span> <span class=\"string\">webserver</span></span><br><span class=\"line\">    <span class=\"attr\">template:</span></span><br><span class=\"line\">        <span class=\"attr\">metadata:</span></span><br><span class=\"line\">            <span class=\"attr\">labels:</span></span><br><span class=\"line\">                <span class=\"attr\">role:</span> <span class=\"string\">webserver</span></span><br><span class=\"line\">        <span class=\"attr\">spec:</span></span><br><span class=\"line\">            <span class=\"attr\">containers:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">frontend</span></span><br><span class=\"line\">              <span class=\"attr\">image:</span> <span class=\"string\">nginx</span>  <span class=\"comment\"># we use wrong image for the demo !</span></span><br><span class=\"line\">              <span class=\"attr\">ports:</span></span><br><span class=\"line\">                <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">nginx-port</span></span><br><span class=\"line\">                  <span class=\"attr\">containerPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\">              <span class=\"attr\">volumeMounts:</span></span><br><span class=\"line\">                <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">nfs</span></span><br><span class=\"line\">                  <span class=\"attr\">mountPath:</span> <span class=\"string\">/usr/share/nginx/html</span></span><br><span class=\"line\">            <span class=\"attr\">volumes:</span></span><br><span class=\"line\">            <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">nfs</span></span><br><span class=\"line\">              <span class=\"attr\">persistentVolumeClaim:</span></span><br><span class=\"line\">                <span class=\"attr\">claimName:</span> <span class=\"string\">pvc-nfs</span></span><br></pre></td></tr></table></figure>\n<p>Deploy the  <code>nginx-deployment.yaml</code>  using the  <code>kubectl apply -f</code> .</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl apply -f nginx-deployment.yaml</span><br><span class=\"line\">deployment.apps/nginx-deployment created</span><br></pre></td></tr></table></figure>\n<p>Make sure that the deployment was created without any problems!</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl get pods</span><br><span class=\"line\">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">nginx-deployment-7976956b49-fgbb4   1/1     Running   0          16s</span><br><span class=\"line\">nginx-deployment-7976956b49-hzrmm   1/1     Running   0          16s</span><br><span class=\"line\">nginx-deployment-7976956b49-kg5tx   1/1     Running   0          16s</span><br></pre></td></tr></table></figure>\n<h3 id=\"sanity-check-testing-the-nfs-volumes\"><a class=\"markdownIt-Anchor\" href=\"#sanity-check-testing-the-nfs-volumes\">#</a> Sanity Check (Testing the NFS volumes):</h3>\n<p>Let’s get shell on one of the running containers and go to the mount point then create a file!</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl <span class=\"built_in\">exec</span> --stdin --<span class=\"built_in\">tty</span> nginx-deployment-7976956b49-fgbb4 -- /bin/bash Command Line Prompt</span><br><span class=\"line\">root@nginx-deployment-7976956b49-fgbb4:/<span class=\"comment\"># ls</span></span><br><span class=\"line\">bin  boot  dev  docker-entrypoint.d  docker-entrypoint.sh  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br></pre></td></tr></table></figure>\n<p>Now the shell is opened. Let’s create a file in  <code>/usr/share/nginx/html</code> :</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@nginx-deployment-7976956b49-fgbb4:/<span class=\"comment\"># cd /usr/share/nginx/html/</span></span><br></pre></td></tr></table></figure>\n<p>We find the file created in the client test xD</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@nginx-deployment-7976956b49-fgbb4:/usr/share/nginx/html<span class=\"comment\"># ls</span></span><br><span class=\"line\">file</span><br></pre></td></tr></table></figure>\n<p>Create a file named “hi from the other side!”</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@nginx-deployment-7976956b49-fgbb4:/usr/share/nginx/html<span class=\"comment\"># touch &quot;hi from the other side!&quot;</span></span><br><span class=\"line\">root@nginx-deployment-7976956b49-fgbb4:/usr/share/nginx/html<span class=\"comment\"># ls</span></span><br><span class=\"line\">file  <span class=\"string\">&#x27;hi from the other side!&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>Let’s open another shell on another running container:</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl <span class=\"built_in\">exec</span> --stdin --<span class=\"built_in\">tty</span> nginx-deployment-7976956b49-kg5tx -- /bin/bash Command Line Prompt</span><br><span class=\"line\">root@nginx-deployment-7976956b49-kg5tx:/<span class=\"comment\"># cd /usr/share/nginx/html/</span></span><br></pre></td></tr></table></figure>\n<p>Bingoo! we find the same content on the same share point!</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@nginx-deployment-7976956b49-kg5tx:/usr/share/nginx/html<span class=\"comment\"># ls</span></span><br><span class=\"line\"> file  <span class=\"string\">&#x27;hi from the other side!&#x27;</span></span><br><span class=\"line\">root@nginx-deployment-7976956b49-kg5tx:/usr/share/nginx/html<span class=\"comment\"># exit</span></span><br></pre></td></tr></table></figure>\n<p>Now we will try to delete the deployment and recreate another to check the data in the  persistent volume</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl delete -f nginx-deployment.yaml</span><br><span class=\"line\">deployment.apps <span class=\"string\">&quot;nginx-deployment&quot;</span> deleted</span><br><span class=\"line\"></span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl apply -f nginx-deployment.yaml</span><br><span class=\"line\">deployment.apps/nginx-deployment created</span><br></pre></td></tr></table></figure>\n<p>Check the deployment created or not !</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~ kubectl get pods</span><br><span class=\"line\">NAME                                READY   STATUS              RESTARTS   AGE</span><br><span class=\"line\">nginx-deployment-7976956b49-7d5vw   1/1     Running             0          10s</span><br><span class=\"line\">nginx-deployment-7976956b49-9r5gx   0/1     ContainerCreating   0          10s</span><br><span class=\"line\">nginx-deployment-7976956b49-fdq7w   1/1     Running             0          10s</span><br></pre></td></tr></table></figure>\n<p>Open another shell on running container from the new deployment to check the content of the persistent volume:</p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl <span class=\"built_in\">exec</span> --stdin --<span class=\"built_in\">tty</span> nginx-deployment-7976956b49-7d5vw -- /bin/bash Command Line Prompt</span><br><span class=\"line\">root@nginx-deployment-7976956b49-7d5vw:/<span class=\"comment\"># cd /usr/share/nginx/html/</span></span><br></pre></td></tr></table></figure>\n<p>Display the content of the mount point  <code>/usr/share/nginx/html/</code></p>\n<figure class=\"highlight bash\"><figcaption><span>Command Line Prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">root@nginx-deployment-7976956b49-7d5vw:/usr/share/nginx/html<span class=\"comment\"># ls</span></span><br><span class=\"line\"> file  <span class=\"string\">&#x27;hi from the other side!&#x27;</span></span><br><span class=\"line\">root@nginx-deployment-7976956b49-7d5vw:/usr/share/nginx/html<span class=\"comment\"># exit</span></span><br><span class=\"line\"><span class=\"built_in\">exit</span></span><br></pre></td></tr></table></figure>\n<p>Bingoo! The content still in the persistent volume without any problem !</p>\n",
            "tags": []
        },
        {
            "id": "http://example.com/2022/09/09/Kubernetes/nginx-ingress/",
            "url": "http://example.com/2022/09/09/Kubernetes/nginx-ingress/",
            "title": "Deploy Nginx Ingress in Bare-metal Cluster",
            "date_published": "2022-09-09T14:08:22.000Z",
            "content_html": "<h1 id=\"ingress-in-bare-metal-kubernetes-cluster\"><a class=\"markdownIt-Anchor\" href=\"#ingress-in-bare-metal-kubernetes-cluster\">#</a> Ingress in bare metal Kubernetes cluster</h1>\n<p>You are working in enterprise that have one cluster with one public IP address and one Domain Name. In our Example  <code>justk8s.k8s</code> .<br>\nThat Enterprise want to deploy two web apps for these new service (gym and restaurant) with that single cluster,IP and Domain Name !!<br>\nYour task is to deploy these two apps! So you decide to use  <code>Ingress</code>  because it’s the best solution for this kind of problems!</p>\n<h2 id=\"what-is-ingress\"><a class=\"markdownIt-Anchor\" href=\"#what-is-ingress\">#</a> What is Ingress</h2>\n<p>Kubernetes Ingress is an API object that provides routing rules to manage external users’ access to the services in a Kubernetes cluster, typically via HTTPS/HTTP. With Ingress, you can easily set up rules for routing traffic without creating a bunch of Load Balancers or exposing each service on the node<br>\n<img data-src=\"https://imgur.com/rXoZYym.png\" alt=\"\"></p>\n<h2 id=\"ingress-controller\"><a class=\"markdownIt-Anchor\" href=\"#ingress-controller\">#</a> Ingress Controller</h2>\n<p>An Ingress Controller is a Kubernetes controller that is deployed manually to the cluster, most often as a DaemonSet or a Deployment object that runs dedicated Pods for handling incoming traffic load balancing and smart routing. It is responsible for processing the Ingress objects (which specify that they especially want to use the Ingress Controller) and dynamically configuring real routing rules<br>\nThe most common used Ingress controllerfor Kubernetes is  <code>Ingress Nginx</code> <br>\n<img data-src=\"https://imgur.com/P0IubVc.png\" alt=\"\"></p>\n<h2 id=\"ingress-in-cloud-vs-ingress-in-bare-metal-cluster\"><a class=\"markdownIt-Anchor\" href=\"#ingress-in-cloud-vs-ingress-in-bare-metal-cluster\">#</a> Ingress in cloud Vs Ingress in bare metal cluster</h2>\n<p>Deploying Ingress pn managed Kubernetes cluster provided in cloud easier than in bare metal cluster. Because the Ingress Controller must be exposed as LoadBlancer!</p>\n<p>In traditional cloud environments, where network load balancers are available on-demand, a single Kubernetes manifest suffices to provide a single point of contact to the NGINX Ingress controller to external clients and, indirectly, to any application running inside the cluster.</p>\n<p><img data-src=\"https://imgur.com/JM86JeT.png\" alt=\"\"></p>\n<p>Bare-metal environments lack this commodity, requiring a slightly different setup to offer the same kind of access to external consumers. So we need an aproach to solve this problem !</p>\n<p><img data-src=\"https://imgur.com/MaTmKew.png\" alt=\"\"></p>\n<h1 id=\"create-load-balancer-proxy-for-bare-metal-cluster\"><a class=\"markdownIt-Anchor\" href=\"#create-load-balancer-proxy-for-bare-metal-cluster\">#</a> Create Load Balancer / Proxy for bare metal cluster</h1>\n<p>There is a lot of method to set up a Load Balancer or Proxy to get Ingress work on our cluster.</p>\n<ul>\n<li>We can expose our ingress controller using NodePort service and set up a proxy that forward to this service using a nginx proxy or HA proxy</li>\n<li>We can also install a software solution called  <code>metalLB</code> . This solution allows to create Load Balancer in local cluster without any problem ! So we can expose our ingress controller using Load Balancer service like in the cloud environment!</li>\n</ul>\n<h2 id=\"create-load-balancer-using-metallb\"><a class=\"markdownIt-Anchor\" href=\"#create-load-balancer-using-metallb\">#</a> Create Load Balancer using metalLB:</h2>\n<p>In this demo we will deploy one of the 2 apps and expose it with Load Balancer service for the test purposes. So Let’s start deploying the restaurant app and expose it !</p>\n<figure class=\"highlight yaml\"><figcaption><span>Restaurant App Deployment</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">4n6nk8s-kitchen</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">replicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">matchLabels:</span></span><br><span class=\"line\">      <span class=\"attr\">app:</span> <span class=\"string\">kitchen</span></span><br><span class=\"line\">  <span class=\"attr\">template:</span></span><br><span class=\"line\">    <span class=\"attr\">metadata:</span></span><br><span class=\"line\">      <span class=\"attr\">labels:</span></span><br><span class=\"line\">        <span class=\"attr\">app:</span> <span class=\"string\">kitchen</span></span><br><span class=\"line\">    <span class=\"attr\">spec:</span></span><br><span class=\"line\">      <span class=\"attr\">containers:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">kitchen-app</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">mohamedrafraf/k8s-kitchen</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">containerPort:</span> <span class=\"number\">80</span></span><br></pre></td></tr></table></figure>\n<p>Create the gym app deployment</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f kitchen-deployment.yaml</span><br><span class=\"line\">deployment.apps/4n6nk8s-kitchen created</span><br><span class=\"line\"></span><br><span class=\"line\">raf²@4n6nk8s-master$ kubectl get all</span><br><span class=\"line\">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">pod/4n6nk8s-kitchen-698f44db99-29dnr   1/1     Running   0          63s</span><br></pre></td></tr></table></figure>\n<p>Let’s now expose now the deployment:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl expose deploy/4n6nk8s-kitchen--type=LoadBalacner --port=80</span><br><span class=\"line\">service/4n6nk8s-kitchen created</span><br></pre></td></tr></table></figure>\n<p>Let’s check this service</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get svc</span><br><span class=\"line\">NAME              TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">4n6nk8s-kitchen   LoadBalancer   10.97.71.78      &lt;pending&gt;       80:30918/TCP   21s</span><br><span class=\"line\">kubernetes        ClusterIP      10.96.0.1        &lt;none&gt;          443/TCP        5d11h</span><br></pre></td></tr></table></figure>\n<p>As we see the LoadBalancer service still pending since we don’t have any Load Balancer! Now it’s time to talk about the metalLB !</p>\n<p>MetalLB provides a network load-balancer implementation for Kubernetes clusters that do not run on a supported cloud provider, effectively allowing the usage of LoadBalancer Services within any cluster.<br>\n<img data-src=\"https://imgur.com/Xv7PiiK.png\" alt=\"\"></p>\n<p>It’s time to install metalLB. Installing metalLB is so easy we just need to apply two manifest from the official site of <span class=\"exturl\" data-url=\"aHR0cHM6Ly9tZXRhbGxiLnVuaXZlcnNlLnRmL2luc3RhbGxhdGlvbi8=\">metalLB</span></p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml</span><br><span class=\"line\">kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml</span><br></pre></td></tr></table></figure>\n<p>After running the previous commands, a new namespace will be created with a deployment, daesmonset and lot of RBAC authorization and rules!. So let’s check this namespace!</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get all -n metallb-system</span><br><span class=\"line\">NAME                              READY   STATUS    RESTARTS       AGE</span><br><span class=\"line\">pod/controller-7cf77c64fb-4b8sx   1/1     Running   0              23s</span><br><span class=\"line\">pod/speaker-chh2t                 1/1     Running   0              23s</span><br><span class=\"line\">pod/speaker-xbn8z                 1/1     Running   0              23s</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE</span><br><span class=\"line\">daemonset.apps/speaker   2         2         2       2            2           kubernetes.io/os=linux   23s</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                         READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class=\"line\">deployment.apps/controller   1/1     1            1           23s</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                    DESIRED   CURRENT   READY   AGE</span><br><span class=\"line\">replicaset.apps/controller-7cf77c64fb   1         1         1       23s</span><br></pre></td></tr></table></figure>\n<h2 id=\"configure-the-address-pool-of-the-load-balancer\"><a class=\"markdownIt-Anchor\" href=\"#configure-the-address-pool-of-the-load-balancer\">#</a> Configure the address pool of the Load Balancer</h2>\n<p>Now we must configure the  <code>metalLB</code>  using configmap object. In this configmap we give the address pool that will be used as LoadBalancer IP! Also the type of the Load Balancer.</p>\n<blockquote>\n<p>metalLB supports two LoadBalancing modes : Layer 2 mode and BGP mode. In our case we will use the Layer 2 mode</p>\n</blockquote>\n<p>Now Let’s move to the configmap manifest!</p>\n<figure class=\"highlight yaml\"><figcaption><span>Configmap</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">ConfigMap</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">metallb-system</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">config</span></span><br><span class=\"line\"><span class=\"attr\">data:</span></span><br><span class=\"line\">  <span class=\"attr\">config:</span> <span class=\"string\">|</span></span><br><span class=\"line\"><span class=\"string\">    address-pools:</span></span><br><span class=\"line\"><span class=\"string\">    - name: default</span></span><br><span class=\"line\"><span class=\"string\">      protocol: layer2</span></span><br><span class=\"line\"><span class=\"string\">      addresses:</span></span><br><span class=\"line\"><span class=\"string\">      - 192.168.1.240-192.168.1.250</span></span><br></pre></td></tr></table></figure>\n<p>After Applying this recent configmap. we can say that the metalLB installtion is finished! So Let’s check the services again in our cluster !</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get svc</span><br><span class=\"line\">NAME              TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGE</span><br><span class=\"line\">4n6nk8s-kitchen   LoadBalancer   10.97.71.78      192.168.1.241   80:30918/TCP   5m4s</span><br><span class=\"line\">kubernetes        ClusterIP      10.96.0.1        &lt;none&gt;          443/TCP        5d11h</span><br></pre></td></tr></table></figure>\n<p>Bingo! our Load Balancer service has an external IP address. So our deployment is accessable through this IP! Let’s check it:<br>\nٍ<img data-src=\"https://imgur.com/jWkr5kf.png\" alt=\"\"><br>\nEverything is OK! now we can say that we have a Load Balancer on our bare metal cluster!</p>\n<h1 id=\"install-the-nginx-ingress-controller\"><a class=\"markdownIt-Anchor\" href=\"#install-the-nginx-ingress-controller\">#</a> Install the nginx Ingress Controller:</h1>\n<p>Installing the nginx ingress controller can be done either by using Helm or the yaml manifest provided in the nginx controller website</p>\n<h2 id=\"install-the-controller-using-helm\"><a class=\"markdownIt-Anchor\" href=\"#install-the-controller-using-helm\">#</a> Install the controller using Helm</h2>\n<p>If you have Helm, you can deploy the ingress controller with the following command:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ helm upgrade --install ingress-nginx ingress-nginx \\</span><br><span class=\"line\">  --repo https://kubernetes.github.io/ingress-nginx \\</span><br><span class=\"line\">  --namespace ingress-nginx --create-namespace</span><br></pre></td></tr></table></figure>\n<h2 id=\"install-the-controller-using-yaml-manifest\"><a class=\"markdownIt-Anchor\" href=\"#install-the-controller-using-yaml-manifest\">#</a> Install the controller using yaml manifest:</h2>\n<p>If you don’t have Helm or if you prefer to use a YAML manifest, you can run the following command instead:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/cloud/deploy.yaml</span><br><span class=\"line\"></span><br><span class=\"line\">namespace/ingress-nginx created</span><br><span class=\"line\">serviceaccount/ingress-nginx created</span><br><span class=\"line\">serviceaccount/ingress-nginx-admission created</span><br><span class=\"line\">role.rbac.authorization.k8s.io/ingress-nginx created</span><br><span class=\"line\">role.rbac.authorization.k8s.io/ingress-nginx-admission created</span><br><span class=\"line\">clusterrole.rbac.authorization.k8s.io/ingress-nginx created</span><br><span class=\"line\">clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created</span><br><span class=\"line\">rolebinding.rbac.authorization.k8s.io/ingress-nginx created</span><br><span class=\"line\">rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created</span><br><span class=\"line\">clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created</span><br><span class=\"line\">clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created</span><br><span class=\"line\">configmap/ingress-nginx-controller created</span><br><span class=\"line\">service/ingress-nginx-controller created</span><br><span class=\"line\">service/ingress-nginx-controller-admission created</span><br><span class=\"line\">deployment.apps/ingress-nginx-controller created</span><br><span class=\"line\">job.batch/ingress-nginx-admission-create created</span><br><span class=\"line\">job.batch/ingress-nginx-admission-patch created</span><br><span class=\"line\">ingressclass.networking.k8s.io/nginx created</span><br><span class=\"line\">validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created</span><br></pre></td></tr></table></figure>\n<h2 id=\"check-the-nginx-ingress-controller\"><a class=\"markdownIt-Anchor\" href=\"#check-the-nginx-ingress-controller\">#</a> Check the Nginx Ingress Controller:</h2>\n<p>Let’s check the pods created in the namespace of the ingress-nginx</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get pods --namespace=ingress-nginx</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                        READY   STATUS              RESTARTS   AGE</span><br><span class=\"line\">ingress-nginx-admission-create-qd8fz        0/1     ContainerCreating   0          10s</span><br><span class=\"line\">ingress-nginx-admission-patch-76lgl         0/1     ContainerCreating   0          10s</span><br><span class=\"line\">ingress-nginx-controller-7575567f98-hg6tq   0/1     ContainerCreating   0          10s</span><br></pre></td></tr></table></figure>\n<p>After waiting a few minute until the controller is ready, we must check all the services and deployments of the ingress-nginx namespace again ! You must find an output similar to the following</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get all -n ingress-nginx</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                            READY   STATUS      RESTARTS        AGE</span><br><span class=\"line\">pod/ingress-nginx-admission-create-9dshm        0/1     Completed   0               23m</span><br><span class=\"line\">pod/ingress-nginx-admission-patch-rj7gk         0/1     Completed   0               23m</span><br><span class=\"line\">pod/ingress-nginx-controller-7575567f98-8w6pm   1/1     Running     0               23m</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                         TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE</span><br><span class=\"line\">service/ingress-nginx-controller             LoadBalancer   10.100.234.221   192.168.1.240   80:32545/TCP,443:31214/TCP   23m</span><br><span class=\"line\">service/ingress-nginx-controller-admission   ClusterIP      10.109.162.157   &lt;none&gt;          443/TCP                      23m</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class=\"line\">deployment.apps/ingress-nginx-controller   1/1     1            1           23m</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class=\"line\">replicaset.apps/ingress-nginx-controller-7575567f98   1         1         1       23m</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                       COMPLETIONS   DURATION   AGE</span><br><span class=\"line\">job.batch/ingress-nginx-admission-create   1/1           3s         23m</span><br><span class=\"line\">job.batch/ingress-nginx-admission-patch    1/1           4s         23m</span><br></pre></td></tr></table></figure>\n<p>Bingo! the ingress controller is ready and on running state! also is exposed with Load Balancer and it have a Extenal-IP  <code>192.168.1.240</code> <br>\nNow we can deploy an ingress object without any problem ! It’s time to deploy our 2 apps with ingress in one single domain name and one IP address !</p>\n<h1 id=\"deploy-the-two-apps-using-ingress\"><a class=\"markdownIt-Anchor\" href=\"#deploy-the-two-apps-using-ingress\">#</a> Deploy The two Apps using Ingress:</h1>\n<p>Let’s start by deploying the apps and expose it with Cluster IP service !</p>\n<figure class=\"highlight yaml\"><figcaption><span>Gym App Deployment</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">4n6nk8s-gym</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">replicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">matchLabels:</span></span><br><span class=\"line\">      <span class=\"attr\">app:</span> <span class=\"string\">gym</span></span><br><span class=\"line\">  <span class=\"attr\">template:</span></span><br><span class=\"line\">    <span class=\"attr\">metadata:</span></span><br><span class=\"line\">      <span class=\"attr\">labels:</span></span><br><span class=\"line\">        <span class=\"attr\">app:</span> <span class=\"string\">gym</span></span><br><span class=\"line\">    <span class=\"attr\">spec:</span></span><br><span class=\"line\">      <span class=\"attr\">containers:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">gym-app</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">mohamedrafraf/k8s-gym</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">containerPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">gym-svc</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">app:</span> <span class=\"string\">gym</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">ClusterIP</span></span><br><span class=\"line\">  <span class=\"attr\">ports:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">protocol:</span> <span class=\"string\">TCP</span></span><br><span class=\"line\">    <span class=\"attr\">port:</span> <span class=\"number\">80</span></span><br></pre></td></tr></table></figure>\n<p>Create the deployment and service of the gym app</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f gym-deployment.yaml</span><br><span class=\"line\">deployment.apps/4n6nk8s-gym created</span><br><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f gym-svc.yaml</span><br><span class=\"line\">service/gym-svc created</span><br></pre></td></tr></table></figure>\n<p>The manifest of the restaurant app and the service:</p>\n<figure class=\"highlight yaml\"><figcaption><span>Restaurant App Deployment</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">apps/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Deployment</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">4n6nk8s-kitchen</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">replicas:</span> <span class=\"number\">1</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">matchLabels:</span></span><br><span class=\"line\">      <span class=\"attr\">app:</span> <span class=\"string\">kitchen</span></span><br><span class=\"line\">  <span class=\"attr\">template:</span></span><br><span class=\"line\">    <span class=\"attr\">metadata:</span></span><br><span class=\"line\">      <span class=\"attr\">labels:</span></span><br><span class=\"line\">        <span class=\"attr\">app:</span> <span class=\"string\">kitchen</span></span><br><span class=\"line\">    <span class=\"attr\">spec:</span></span><br><span class=\"line\">      <span class=\"attr\">containers:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">kitchen-app</span></span><br><span class=\"line\">        <span class=\"attr\">image:</span> <span class=\"string\">mohamedrafraf/k8s-kitchen</span></span><br><span class=\"line\">        <span class=\"attr\">ports:</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">containerPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">kitchen-svc</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">app:</span> <span class=\"string\">kitchen</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">ClusterIP</span></span><br><span class=\"line\">  <span class=\"attr\">ports:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">protocol:</span> <span class=\"string\">TCP</span></span><br><span class=\"line\">    <span class=\"attr\">port:</span> <span class=\"number\">80</span></span><br></pre></td></tr></table></figure>\n<p>Create the restaurant deployment and the service:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f kitchen-deployment.yaml</span><br><span class=\"line\">deployment.apps/4n6nk8s-kitchen created</span><br><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f kitchen-svc.yaml</span><br><span class=\"line\">service/kitchen-svc created</span><br></pre></td></tr></table></figure>\n<p>Check the deployments and the services! :</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get all</span><br><span class=\"line\">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">pod/4n6nk8s-gym-698f44db99-29dnr       1/1     Running   0          63s</span><br><span class=\"line\">pod/4n6nk8s-kitchen-6b56959b86-n7c9z   1/1     Running   0          46s</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE</span><br><span class=\"line\">service/gym-svc             ClusterIP   10.110.191.206   &lt;none&gt;        80/TCP     53s</span><br><span class=\"line\">service/kitchen-svc         ClusterIP   10.102.94.94     &lt;none&gt;        80/TCP     39s</span><br><span class=\"line\">service/kubernetes          ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP    3d14h</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                              READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class=\"line\">deployment.apps/4n6nk8s-gym       1/1     1            1           63s</span><br><span class=\"line\">deployment.apps/4n6nk8s-kitchen   1/1     1            1           46s</span><br><span class=\"line\"></span><br><span class=\"line\">NAME                                         DESIRED   CURRENT   READY   AGE</span><br><span class=\"line\">replicaset.apps/4n6nk8s-gym-698f44db99       1         1         1       63s</span><br><span class=\"line\">replicaset.apps/4n6nk8s-kitchen-6b56959b86   1         1         1       46s</span><br></pre></td></tr></table></figure>\n<p>Let’s now create the ingress manifest:</p>\n<figure class=\"highlight yaml\"><figcaption><span>Ingress YAML Definition</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">networking.k8s.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Ingress</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">annotations:</span></span><br><span class=\"line\">    <span class=\"attr\">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class=\"string\">/</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">ingress-4n6nk8s</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">ingressClassName:</span> <span class=\"string\">nginx</span></span><br><span class=\"line\">  <span class=\"attr\">rules:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">host:</span> <span class=\"string\">justk8s.k8s</span></span><br><span class=\"line\">    <span class=\"attr\">http:</span></span><br><span class=\"line\">      <span class=\"attr\">paths:</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">path:</span> <span class=\"string\">/kitchen</span></span><br><span class=\"line\">        <span class=\"attr\">pathType:</span> <span class=\"string\">Prefix</span></span><br><span class=\"line\">        <span class=\"attr\">backend:</span></span><br><span class=\"line\">          <span class=\"attr\">service:</span></span><br><span class=\"line\">            <span class=\"attr\">name:</span> <span class=\"string\">kitchen-svc</span></span><br><span class=\"line\">            <span class=\"attr\">port:</span></span><br><span class=\"line\">              <span class=\"attr\">number:</span> <span class=\"number\">80</span></span><br><span class=\"line\">      <span class=\"bullet\">-</span> <span class=\"attr\">path:</span> <span class=\"string\">/gym</span></span><br><span class=\"line\">        <span class=\"attr\">pathType:</span> <span class=\"string\">Prefix</span></span><br><span class=\"line\">        <span class=\"attr\">backend:</span></span><br><span class=\"line\">          <span class=\"attr\">service:</span></span><br><span class=\"line\">            <span class=\"attr\">name:</span> <span class=\"string\">gym-svc</span></span><br><span class=\"line\">            <span class=\"attr\">port:</span></span><br><span class=\"line\">              <span class=\"attr\">number:</span> <span class=\"number\">80</span></span><br></pre></td></tr></table></figure>\n<p>The  <code>ingressClassName</code>  must be “nginx” since we have nginx ingress controller. The  <code>host</code>  field in the  <code>rules</code>  define the domain name of the request. So if a HTTP request come to the cluster with the host name  <code>justk8s.k8s</code>  this rull will be applied!<br>\nThe the  <code>path</code>  int the  <code>paths</code>  will be the route ! The ingress give you the service in this route!<br>\nIn our Example if we enter  <code>http://justk8s.k8s/kitchen</code>  the ingress will forward the restaurant app!<br>\nin the  <code>path</code>  field we specify the  <code>service</code>  name and the  <code>port number</code></p>\n<p>Now Let’s Create the ingress object</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl apply -f ingress.yaml</span><br><span class=\"line\">ingress.networking.k8s.io/ingress-4n6nk8s created</span><br></pre></td></tr></table></figure>\n<p>Check the ingerss object !</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl get ingress</span><br><span class=\"line\">NAME              CLASS   HOSTS         ADDRESS   PORTS   AGE</span><br><span class=\"line\">ingress-4n6nk8s   nginx   justk8s.k8s             80      5s</span><br></pre></td></tr></table></figure>\n<p>Display more details about the ingress object with  <code>kubectl describe ingress</code> :</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf²@4n6nk8s-master$ kubectl describe ingress ingress-4n6nk8s</span><br><span class=\"line\">Name:             ingress-4n6nk8s</span><br><span class=\"line\">Labels:           &lt;none&gt;</span><br><span class=\"line\">Namespace:        default</span><br><span class=\"line\">Address:          192.168.1.240</span><br><span class=\"line\">Default backend:  default-http-backend:80 (&lt;error: endpoints <span class=\"string\">&quot;default-http-backend&quot;</span> not found&gt;)</span><br><span class=\"line\">Rules:</span><br><span class=\"line\">  Host         Path  Backends</span><br><span class=\"line\">  ----         ----  --------</span><br><span class=\"line\">  justk8s.k8s</span><br><span class=\"line\">               /kitchen   kitchen-svc:80 (10.1.235.141:80)</span><br><span class=\"line\">               /gym       gym-svc:80 (10.1.235.147:80)</span><br><span class=\"line\">Annotations:   nginx.ingress.kubernetes.io/rewrite-target: /</span><br><span class=\"line\">Events:</span><br><span class=\"line\">  Type    Reason  Age               From                      Message</span><br><span class=\"line\">  ----    ------  ----              ----                      -------</span><br><span class=\"line\">  Normal  Sync    7s (x2 over 19s)  nginx-ingress-controller  Scheduled <span class=\"keyword\">for</span> <span class=\"built_in\">sync</span></span><br></pre></td></tr></table></figure>\n<p>We see that the Default backend display  <code>(&lt;error: endpoints &quot;default-http-backend&quot; not found&gt;)</code> .<br>\nThis error occur because we don’t specify a default backend (route) so when you enter  <code>http://justk8s.k8s/</code>  we will find the nginx 404 not found page !</p>\n<h1 id=\"check-the-deployments\"><a class=\"markdownIt-Anchor\" href=\"#check-the-deployments\">#</a> Check the Deployments!</h1>\n<p>After Installing the LoadBalancer and Ingress Controller. Let’s try to access to our 2 apps. But before access to this apps let’s create an  <code>/etc/hosts</code>  entry ( <code>C:\\Windows\\System32\\Drivers\\etc\\hosts</code>  for Windows):</p>\n<figure class=\"highlight bash\"><figcaption><span>/etc/hosts</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">192.168.1.240 justk8s.k8s <span class=\"comment\"># The IP address of the Load Balancer of the Ingress Controller !</span></span><br></pre></td></tr></table></figure>\n<p>Now Let’s go to our browser! I add this entry in my master node! So I can access to these apps from my master node !:</p>\n<p><img data-src=\"https://imgur.com/p4sAzQX.png\" alt=\"\"><br>\n<img data-src=\"https://imgur.com/tauie6V.png\" alt=\"\"></p>\n<p>Bingo! Everything is Ok !</p>\n",
            "tags": []
        },
        {
            "id": "http://example.com/2022/09/08/Kubernetes/private-docker-k8s/",
            "url": "http://example.com/2022/09/08/Kubernetes/private-docker-k8s/",
            "title": "Dockerize a website & Pull it privately in k8s",
            "date_published": "2022-09-08T00:47:58.000Z",
            "content_html": "<p>In this article you will learn how to containerize a static website using nginx. Then we will push a private docker image in dockerhub. Finally we will use this private image to be pulled in our Kubernetes cluster ! I will split this article to 2 small and easy steps, you can skip any one you want !</p>\n<h1 id=\"containerize-a-static-website-and-push-it-on-dockerhub\"><a class=\"markdownIt-Anchor\" href=\"#containerize-a-static-website-and-push-it-on-dockerhub\">#</a> Containerize a static website and push it on dockerhub</h1>\n<p>In this Section we will choose a template from random websites that provides free css templates, then we will dockerize it !</p>\n<h2 id=\"dockerize-the-website\"><a class=\"markdownIt-Anchor\" href=\"#dockerize-the-website\">#</a> Dockerize the website</h2>\n<p>I’ll choose this template from this <span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuZnJlZS1jc3MuY29tL2ZyZWUtY3NzLXRlbXBsYXRlcy9wYWdlMjgyL3JveWFsLWNhcnM=\">link</span><br>\n<img data-src=\"https://imgur.com/ngHGyw0.png\" alt=\"\"></p>\n<p>Download it and let’s create our Dockerfile !<br>\nWe will use the  <code>nginx:alpine</code>  image and copy all the assets of the website to the  <code>/usr/share/nginx/html</code>  to be hosted by the nginx webserver.</p>\n<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">FROM</span> nginx:alpine</span><br><span class=\"line\"><span class=\"keyword\">WORKDIR</span><span class=\"language-bash\"> /usr/share/nginx/html</span></span><br><span class=\"line\"><span class=\"keyword\">RUN</span><span class=\"language-bash\"> <span class=\"built_in\">rm</span> -rf ./*</span></span><br><span class=\"line\"><span class=\"keyword\">COPY</span><span class=\"language-bash\"> ./ ./</span></span><br><span class=\"line\"><span class=\"keyword\">RUN</span><span class=\"language-bash\"> <span class=\"built_in\">chmod</span> +r -R . </span></span><br><span class=\"line\"><span class=\"keyword\">ENTRYPOINT</span><span class=\"language-bash\"> [<span class=\"string\">&quot;nginx&quot;</span>,<span class=\"string\">&quot;-g&quot;</span>,<span class=\"string\">&quot;daemon off;&quot;</span>]</span></span><br></pre></td></tr></table></figure>\n<p>Now it’s time to build the container image</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ sudo docker build . -t &lt;user_name&gt;/cars-app</span><br><span class=\"line\">Sending build context to Docker daemon  2.454MB</span><br><span class=\"line\">Step 1/5 : FROM nginx:alpine</span><br><span class=\"line\">alpine: Pulling from library/nginx</span><br><span class=\"line\">213ec9aee27d: Downloading [===================================&gt;               ]  2.018MB/2.806MB</span><br><span class=\"line\">2546ae67167b: Downloading [=========&gt;                                         ]  1.461MB/7.403MB</span><br><span class=\"line\">23b845224e13: Download complete</span><br><span class=\"line\">9bd5732789a3: Download complete</span><br><span class=\"line\">328309e59ded: Waiting</span><br><span class=\"line\">b231d02e5150: Waiting</span><br></pre></td></tr></table></figure>\n<p>Now let’s create a container to test it before make push it!</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ sudo docker run --name car-demo -p 8686:80 &lt;user_name&gt;/cars-app</span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: using the &quot;epoll&quot; event method</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: nginx/1.23.1</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: built by gcc 11.2.1 20220219 (Alpine 11.2.1_git20220219)</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: OS: Linux 5.15.0-41-generic</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: start worker processes</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: start worker process 7</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: start worker process 8</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: start worker process 9</span></span><br><span class=\"line\">2022/09/08 22:53:47 [notice] 1<span class=\"comment\">#1: start worker process 10</span></span><br></pre></td></tr></table></figure>\n<p>Sanity Check please ! Oh everything is OK !</p>\n<p><img data-src=\"https://imgur.com/nQ6NcxG.png\" alt=\"\"></p>\n<h2 id=\"push-the-container-image-to-private-repo\"><a class=\"markdownIt-Anchor\" href=\"#push-the-container-image-to-private-repo\">#</a> Push the container image to private repo</h2>\n<p>Now go and create a private repository in your dockerhub to push it !</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ sudo docker image push &lt;user_name&gt;/cars-app</span><br><span class=\"line\">Using default tag: latest</span><br><span class=\"line\">The push refers to repository [docker.io/&lt;user_name&gt;/cars-app]</span><br><span class=\"line\">41c30355eff8: Pushed</span><br><span class=\"line\">f2ba5e032e84: Pushed</span><br><span class=\"line\">43e1f37b87cb: Pushed</span><br><span class=\"line\">bf4e176a4d9b: Mounted from library/nginx</span><br><span class=\"line\">a1d571e4e83d: Mounted from library/nginx</span><br><span class=\"line\">6d97b4d00719: Mounted from library/nginx</span><br><span class=\"line\">2a7647ca3937: Mounted from library/nginx</span><br><span class=\"line\">549c42eea4a6: Mounted from library/nginx</span><br><span class=\"line\">994393dc58e7: Mounted from library/nginx</span><br></pre></td></tr></table></figure>\n<p>Now it’s time to deal with our kubernetes cluster !</p>\n<h1 id=\"use-the-private-docker-image-in-kubernetes\"><a class=\"markdownIt-Anchor\" href=\"#use-the-private-docker-image-in-kubernetes\">#</a> Use the private docker image in Kubernetes</h1>\n<p>In this demo I will use a production kubernetes cluster with 1 master node and 1 worker node</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl get nodes</span><br><span class=\"line\">NAME      STATUS   ROLES                  AGE   VERSION</span><br><span class=\"line\">master    Ready    control-plane,master   77d   v1.23.1</span><br><span class=\"line\">worker1   Ready    &lt;none&gt;                 77d   v1.23.1</span><br></pre></td></tr></table></figure>\n<p>Let’s create a namespace for this demo !</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl create ns private-docker</span><br><span class=\"line\">namespace/private-docker created</span><br></pre></td></tr></table></figure>\n<p>To make our cluster pull private images we need to create a special secret object with specific type called  <code>docker-registry</code>  secret. To make this secret work correctly you must specify the docker registry, username , password and docker email!</p>\n<p>I prefer to put this params in variable environment to work with it easly</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> EMAIL=&lt;email&gt;</span><br><span class=\"line\"><span class=\"built_in\">export</span> USERNAME=&lt;user_name&gt;</span><br><span class=\"line\"><span class=\"built_in\">export</span> PASSWORD=&lt;password&gt;</span><br><span class=\"line\"><span class=\"built_in\">export</span> SERVER=&lt;docker_registry&gt;</span><br></pre></td></tr></table></figure>\n<p>In case you will use dockerhub as your registry you don’t have to specify the server !</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl create -n private-docker secret docker-registry docker-secret --docker-username=$USERNAME --docker-password=$PASSWORD --docker-email=$EMAIL</span><br><span class=\"line\">secret/docker-secret created</span><br></pre></td></tr></table></figure>\n<p>Now let’s create a pod with the image that we made it !</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Pod</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">labels:</span></span><br><span class=\"line\">    <span class=\"attr\">run:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">private-docker</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">image:</span> <span class=\"string\">&lt;user_name&gt;/cars-app</span></span><br><span class=\"line\">    <span class=\"attr\">name:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">    <span class=\"attr\">resources:</span> &#123;&#125;</span><br><span class=\"line\">  <span class=\"attr\">dnsPolicy:</span> <span class=\"string\">ClusterFirst</span></span><br><span class=\"line\">  <span class=\"attr\">restartPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>If you try create this pod with this YAML definition you’ll get a  <code>ErrImagePull</code>  because we don’t specify the docker secret that we created</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl get pods -n private-docker</span><br><span class=\"line\">NAME           READY   STATUS         RESTARTS   AGE</span><br><span class=\"line\">cars-app-pod   0/1     ErrImagePull   0          5s</span><br></pre></td></tr></table></figure>\n<p>Let’s figure out the problem with  <code>kubectl describe</code>  command. Take a look at the events that occur when the pod try to pull the container image!</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Events:</span><br><span class=\"line\"> Type     Reason          Age                From               Message</span><br><span class=\"line\"> ----     ------          ----               ----               -------</span><br><span class=\"line\"> Normal   Scheduled       44s                default-scheduler  Successfully assigned private-docker/cars-app-pod to worker1</span><br><span class=\"line\"> Normal   SandboxChanged  40s                kubelet            Pod sandbox changed, it will be killed and re-created.</span><br><span class=\"line\"> Normal   Pulling         26s (x2 over 43s)  kubelet            Pulling image &quot;&lt;user_name&gt;/cars-app&quot;</span><br><span class=\"line\"> Warning  Failed          23s (x2 over 40s)  kubelet            Failed to pull image &quot;&lt;user_name&gt;/cars-app&quot;: rpc error: code = Unknown desc = Error response from daemon: pull access denied for &lt;user_name&gt;/cars-app, repository does not exist or may require &#x27;docker login&#x27;: denied: requested access to the resource is denied</span><br><span class=\"line\"> Warning  Failed          23s (x2 over 40s)  kubelet            Error: ErrImagePull</span><br><span class=\"line\"> Normal   BackOff         12s (x4 over 39s)  kubelet            Back-off pulling image &quot;&lt;user_name&gt;/cars-app&quot;</span><br><span class=\"line\"> Warning  Failed          12s (x4 over 39s)  kubelet            Error: ImagePullBackOff</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Specifying the docker secret in the pod YAML definition will solve this problem. The pod will pull the container image without any problem<br>\nAdding  <code>imagePullSecrets</code>  attributes in the  <code>spec</code>  of the pod allow the pod to pull this image. The Final YAML definition will be similar to the following one !</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Pod</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">labels:</span></span><br><span class=\"line\">    <span class=\"attr\">run:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">private-docker</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">image:</span> <span class=\"string\">&lt;user_name&gt;/cars-app</span></span><br><span class=\"line\">    <span class=\"attr\">name:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">    <span class=\"attr\">resources:</span> &#123;&#125;</span><br><span class=\"line\">  <span class=\"attr\">imagePullSecrets:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">docker-secret</span></span><br><span class=\"line\">  <span class=\"attr\">dnsPolicy:</span> <span class=\"string\">ClusterFirst</span></span><br><span class=\"line\">  <span class=\"attr\">restartPolicy:</span> <span class=\"string\">Always</span></span><br><span class=\"line\"><span class=\"attr\">status:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>Create the pod again and check it with  <code>kubectl get pods</code>  and don’t forget to specify the namespace!</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl get pods -n private-docker</span><br><span class=\"line\">NAME           READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">cars-app-pod   1/1     Running   0          4s</span><br></pre></td></tr></table></figure>\n<p>I want to make sure that everything is ok, so i will expose this pod using the NodePort service. The following service YAML Definition will expose the pod correctly :</p>\n<figure class=\"highlight yaml\"><figcaption><span>yaml</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Service</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">creationTimestamp:</span> <span class=\"literal\">null</span></span><br><span class=\"line\">  <span class=\"attr\">labels:</span></span><br><span class=\"line\">    <span class=\"attr\">run:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">namespace:</span> <span class=\"string\">private-docker</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">ports:</span></span><br><span class=\"line\">  <span class=\"bullet\">-</span> <span class=\"attr\">port:</span> <span class=\"number\">80</span></span><br><span class=\"line\">    <span class=\"attr\">protocol:</span> <span class=\"string\">TCP</span></span><br><span class=\"line\">    <span class=\"attr\">targetPort:</span> <span class=\"number\">80</span></span><br><span class=\"line\">  <span class=\"attr\">selector:</span></span><br><span class=\"line\">    <span class=\"attr\">run:</span> <span class=\"string\">cars-app-pod</span></span><br><span class=\"line\">  <span class=\"attr\">type:</span> <span class=\"string\">NodePort</span></span><br></pre></td></tr></table></figure>\n<p>Let’s now check the service and get the node port that we will use it to test the pod</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">raf@4n6nk8s:~$ kubectl get svc -n private-docker</span><br><span class=\"line\">NAME           TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE</span><br><span class=\"line\">cars-app-pod   NodePort   10.111.98.22   &lt;none&gt;        80:31651/TCP   6s</span><br></pre></td></tr></table></figure>\n<p>Now you can visit either <span class=\"exturl\" data-url=\"aHR0cDovL21hc3RlcjozMTY1MQ==\">http://master:31651</span> or <span class=\"exturl\" data-url=\"aHR0cDovL3dvcmtlcjE6MzE2NTE=\">http://worker1:31651</span> and you will find the static website work without any problem!</p>\n<p><img data-src=\"https://imgur.com/3TVOl36.png\" alt=\"\"></p>\n",
            "tags": [
                "kubernetes"
            ]
        },
        {
            "id": "http://example.com/2022/09/06/Kubernetes/configmap-secrets/",
            "url": "http://example.com/2022/09/06/Kubernetes/configmap-secrets/",
            "title": "Introduction To Configmap & Secrets",
            "date_published": "2022-09-06T10:29:37.000Z",
            "content_html": "<h2 id=\"an-introduction-to-kubernetes-secrets-and-configmaps\"><a class=\"markdownIt-Anchor\" href=\"#an-introduction-to-kubernetes-secrets-and-configmaps\">#</a> An introduction to Kubernetes Secrets and ConfigMaps</h2>\n<p>Kubernetes has two types of objects that can inject configuration data into a container when it starts up: Secrets and ConfigMaps. Secrets and ConfigMaps behave similarly in Kubernetes, both in how they are created and because they can be exposed inside a container as mounted files or volumes or environment variables.</p>\n<h3 id=\"to-understand-more-how-configmap-and-secrets-are-important-lets-consider-the-following-scenario\"><a class=\"markdownIt-Anchor\" href=\"#to-understand-more-how-configmap-and-secrets-are-important-lets-consider-the-following-scenario\">#</a> To understand more how configmap and secrets are important let’s consider the following scenario:</h3>\n<p>You have to run a postgres docker image on your host, you explore the documentation of this docker image provided in <em><a href=\"%22https://hub.docker.com/_/postgres%22\">DockerHub</a></em> and you find that you the PostgreSQL image uses several environment variables and there is a mandatory variable called  <code>POSTGRES_PASSWORD</code>  must be defined by running this following command:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$  docker run --name my-postgres -e POSTGRES_PASSWORD=mypassword -d postgres</span><br></pre></td></tr></table></figure>\n<h3 id=\"but-how-we-can-use-this-environment-variables-and-how-we-can-manage-them-in-kubernetes\"><a class=\"markdownIt-Anchor\" href=\"#but-how-we-can-use-this-environment-variables-and-how-we-can-manage-them-in-kubernetes\">#</a> But how we can use this environment variables and how we can manage them in kubernetes ?</h3>\n<p>We can centralize the variables environment in two types of objects and import these variables in the manifest of a pod, replicasets, or deployments</p>\n<h3 id=\"what-is-a-configmaps\"><a class=\"markdownIt-Anchor\" href=\"#what-is-a-configmaps\">#</a> What is a ConfigMaps</h3>\n<p>In Kubernetes, a ConfigMap is nothing more than a key/value pair. A ConfigMap store’s non-confidential data, meaning no passwords or API keys. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.</p>\n<p>A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.</p>\n<h4 id=\"example-of-confimap-manifest-that-store-the-database-name-username\"><a class=\"markdownIt-Anchor\" href=\"#example-of-confimap-manifest-that-store-the-database-name-username\">#</a> Example of confimap manifest that store the database name &amp; username:</h4>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">ConfigMap</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">config-map</span></span><br><span class=\"line\"><span class=\"attr\">data:</span></span><br><span class=\"line\">  <span class=\"attr\">postgres-db:</span> <span class=\"string\">&quot;database&quot;</span></span><br><span class=\"line\">  <span class=\"attr\">postgres-user:</span> <span class=\"string\">&quot;mohamed&quot;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"create-the-configmaps-object\"><a class=\"markdownIt-Anchor\" href=\"#create-the-configmaps-object\">#</a> Create the ConfigMaps Object:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl apply -f first-configmap.yaml</span><br><span class=\"line\">configmap/config-map created</span><br></pre></td></tr></table></figure>\n<h4 id=\"list-the-configmap-objects\"><a class=\"markdownIt-Anchor\" href=\"#list-the-configmap-objects\">#</a> List the ConfigMap Objects:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl get configmap</span><br><span class=\"line\">NAME               DATA   AGE</span><br><span class=\"line\">config-map         2      26s</span><br><span class=\"line\">kube-root-ca.crt   1      13h</span><br></pre></td></tr></table></figure>\n<h3 id=\"what-is-a-secrets\"><a class=\"markdownIt-Anchor\" href=\"#what-is-a-secrets\">#</a> What is a Secrets:</h3>\n<p>Secrets are a Kubernetes object intended for storing a small amount of sensitive data. It is worth noting that Secrets are stored base64-encoded within Kubernetes, so they are not wildly secure.<br>\nSecrets are similar to ConfigMaps but are specifically intended to hold confidential data.</p>\n<h4 id=\"example-of-confimap-manifest-that-store-the-database-password\"><a class=\"markdownIt-Anchor\" href=\"#example-of-confimap-manifest-that-store-the-database-password\">#</a> Example of confimap manifest that store the database password:</h4>\n<p>we must encode the value that we will stored in the Secrets</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ <span class=\"built_in\">echo</span> -n <span class=\"string\">&quot;mohamed&quot;</span> | <span class=\"built_in\">base64</span></span><br><span class=\"line\">bW9oYW1lZA==</span><br></pre></td></tr></table></figure>\n<p>Now we can use the base64 cipher in the Secret manifest</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Secret</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">database-secret</span></span><br><span class=\"line\"><span class=\"attr\">type:</span> <span class=\"string\">Opaque</span></span><br><span class=\"line\"><span class=\"attr\">data:</span></span><br><span class=\"line\">  <span class=\"attr\">postgres-pass:</span> <span class=\"string\">&quot;bW9oYW1lZA==&quot;</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"create-the-configmaps-object-2\"><a class=\"markdownIt-Anchor\" href=\"#create-the-configmaps-object-2\">#</a> Create the ConfigMaps Object:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl apply -f first-secret.yaml</span><br><span class=\"line\">secret/database-secret created</span><br></pre></td></tr></table></figure>\n<h4 id=\"list-the-configmap-objects-2\"><a class=\"markdownIt-Anchor\" href=\"#list-the-configmap-objects-2\">#</a> List the ConfigMap Objects:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl get secrets</span><br><span class=\"line\">NAME                  TYPE                                  DATA   AGE</span><br><span class=\"line\">database-secret       Opaque                                1      8s</span><br><span class=\"line\">default-token-xl8sl   kubernetes.io/service-account-token   3      13h</span><br></pre></td></tr></table></figure>\n<h3 id=\"how-to-use-configmaps-and-secrets-values-in-a-pod\"><a class=\"markdownIt-Anchor\" href=\"#how-to-use-configmaps-and-secrets-values-in-a-pod\">#</a> How to use ConfigMaps and Secrets values in a Pod</h3>\n<p>we can use the values from  <code>ConfigMaps</code>  and  <code>Secrets</code>  in the pod manifests in the  <code>env</code>  propriety of the container by using the  <code>valueFrom</code>  field that can import values from configMap and Secrets</p>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Pod</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">&lt;pod-name&gt;</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">&lt;name&gt;</span></span><br><span class=\"line\">      <span class=\"attr\">image:</span> <span class=\"string\">&lt;image&gt;</span></span><br><span class=\"line\">      <span class=\"attr\">env:</span></span><br><span class=\"line\">        <span class=\"comment\"># Define the environment variable</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">&lt;variable-name&gt;</span></span><br><span class=\"line\">          <span class=\"attr\">valueFrom:</span></span><br><span class=\"line\">            <span class=\"attr\">configMapKeyRef:</span></span><br><span class=\"line\">              <span class=\"attr\">name:</span> <span class=\"string\">&lt;config-map-object&gt;</span>     </span><br><span class=\"line\">              <span class=\"attr\">key:</span> <span class=\"string\">&lt;key-name&gt;</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">&lt;variable-name&gt;</span></span><br><span class=\"line\">          <span class=\"attr\">valueFrom:</span></span><br><span class=\"line\">            <span class=\"attr\">secretKeyRef:</span></span><br><span class=\"line\">              <span class=\"attr\">name:</span> <span class=\"string\">&lt;secret-object&gt;</span>     </span><br><span class=\"line\">              <span class=\"attr\">key:</span> <span class=\"string\">&lt;key-name&gt;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"create-a-postgresql-pod-that-uses-values-from-configmaps-and-secrets\"><a class=\"markdownIt-Anchor\" href=\"#create-a-postgresql-pod-that-uses-values-from-configmaps-and-secrets\">#</a> Create a PostgreSQL Pod that uses values from ConfigMaps and Secrets</h3>\n<h4 id=\"1-create-the-pod-manifest\"><a class=\"markdownIt-Anchor\" href=\"#1-create-the-pod-manifest\">#</a> 1- Create the Pod manifest:</h4>\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Pod</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">postgresql</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"attr\">containers:</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">postgres</span></span><br><span class=\"line\">      <span class=\"attr\">image:</span> <span class=\"string\">postgres</span></span><br><span class=\"line\">      <span class=\"attr\">ports:</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">containerPort:</span> <span class=\"number\">5432</span></span><br><span class=\"line\">      <span class=\"attr\">env:</span></span><br><span class=\"line\">        <span class=\"comment\"># Define the environment variable</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">POSTGRES_USER</span></span><br><span class=\"line\">          <span class=\"attr\">valueFrom:</span></span><br><span class=\"line\">            <span class=\"attr\">configMapKeyRef:</span></span><br><span class=\"line\">              <span class=\"attr\">name:</span> <span class=\"string\">config-map</span>   </span><br><span class=\"line\">              <span class=\"attr\">key:</span> <span class=\"string\">postgres-user</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">POSTGRES_DBNAME</span></span><br><span class=\"line\">          <span class=\"attr\">valueFrom:</span></span><br><span class=\"line\">            <span class=\"attr\">configMapKeyRef:</span></span><br><span class=\"line\">              <span class=\"attr\">name:</span> <span class=\"string\">config-map</span>   </span><br><span class=\"line\">              <span class=\"attr\">key:</span> <span class=\"string\">postgres-db</span></span><br><span class=\"line\">        <span class=\"bullet\">-</span> <span class=\"attr\">name:</span> <span class=\"string\">POSTGRES_PASSWORD</span></span><br><span class=\"line\">          <span class=\"attr\">valueFrom:</span></span><br><span class=\"line\">            <span class=\"attr\">secretKeyRef:</span></span><br><span class=\"line\">              <span class=\"attr\">name:</span> <span class=\"string\">database-secret</span>   </span><br><span class=\"line\">              <span class=\"attr\">key:</span> <span class=\"string\">postgres-pass</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"2-create-the-pod\"><a class=\"markdownIt-Anchor\" href=\"#2-create-the-pod\">#</a> 2- Create the Pod:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl apply -f postgres.yaml</span><br><span class=\"line\">pod/postgresql created</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-list-the-created-pod\"><a class=\"markdownIt-Anchor\" href=\"#3-list-the-created-pod\">#</a> 3- List The Created Pod:</h4>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl get pods</span><br><span class=\"line\"></span><br><span class=\"line\">NAME         READY   STATUS    RESTARTS   AGE</span><br><span class=\"line\">postgresql   1/1     Running   0          8s</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-test-the-database-created-with-variables-of-configmap-and-secrets\"><a class=\"markdownIt-Anchor\" href=\"#4-test-the-database-created-with-variables-of-configmap-and-secrets\">#</a> 4- Test the Database created with variables of ConfigMap and Secrets:</h4>\n<p>We can open a bash session on the pod and open the database  <code>mohamed</code>  with the  <code>psql</code>  command provided by the postgreSQL</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">justk8s@justk8s-master:~$ kubectl <span class=\"built_in\">exec</span> --stdin --<span class=\"built_in\">tty</span> postgresql  -- /bin/bash</span><br><span class=\"line\"></span><br><span class=\"line\">root@postgresql:/<span class=\"comment\"># psql -U &quot;mohamed&quot;</span></span><br><span class=\"line\">psql (14.4 (Debian 14.4-1.pgdg110+1))</span><br><span class=\"line\">Type <span class=\"string\">&quot;help&quot;</span> <span class=\"keyword\">for</span> <span class=\"built_in\">help</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">mohamed=<span class=\"comment\">#</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"references\"><a class=\"markdownIt-Anchor\" href=\"#references\">#</a> References:</h4>\n<p><em><span class=\"exturl\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3MvdGFza3MvZGVidWcvZGVidWctYXBwbGljYXRpb24vZ2V0LXNoZWxsLXJ1bm5pbmctY29udGFpbmVyLw==\">Get a Shell to a Running Container</span></em></p>\n<p><em><span class=\"exturl\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3MvY29uY2VwdHMvY29uZmlndXJhdGlvbi9jb25maWdtYXAv\">ConfigMaps</span></em></p>\n<p><em><span class=\"exturl\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3MvY29uY2VwdHMvY29uZmlndXJhdGlvbi9zZWNyZXQv\">Secrets</span></em></p>\n<p><em><span class=\"exturl\" data-url=\"aHR0cHM6Ly9odWIuZG9ja2VyLmNvbS9fL3Bvc3RncmVz\">PostgreSQL Docker</span></em></p>\n",
            "tags": []
        },
        {
            "id": "http://example.com/2022/08/05/Kubernetes/kubeadm/",
            "url": "http://example.com/2022/08/05/Kubernetes/kubeadm/",
            "title": "Setting Up Kubernetes with Kubeadm",
            "date_published": "2022-08-05T10:54:12.000Z",
            "content_html": "<p><span class=\"red\">Kubeadm</span> is a tool used to build Kubernetes (K8s) clusters. Kubeadm performs the actions necessary to get a minimum viable cluster up and running quickly. By design, it cares only about bootstrapping, not about provisioning machines (underlying worker and master nodes).</p>\n<div class=\"note success\">\n<p>Knowing how to use kubeadm is required for CKA and CKS exams</p>\n</div>\n<p>We configure a 3 <span class=\"pinky\">Ubuntu 20.04 LTS</span> machines in the same network with the following proprietes:</p>\n<table>\n<thead>\n<tr>\n<th>Role</th>\n<th>Hostname</th>\n<th>IP address</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Master</td>\n<td>4n6nk8s-master</td>\n<td>192.168.1.18/24</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>4n6nk8s-worker1</td>\n<td>192.168.1.19/24</td>\n</tr>\n<tr>\n<td>Worker</td>\n<td>4n6nk8s-worker2</td>\n<td>192.168.1.20/24</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>Note: Make sure to setup a unique hostname for each host</p>\n</blockquote>\n<h2 id=\"prepare-the-environments\"><a class=\"markdownIt-Anchor\" href=\"#prepare-the-environments\">#</a> Prepare the environments</h2>\n<p>The following Steps must be applied to each node (both master nodes and worker nodes)</p>\n<h3 id=\"disable-the-swap-memory\"><a class=\"markdownIt-Anchor\" href=\"#disable-the-swap-memory\">#</a> Disable the Swap Memory</h3>\n<p>The Kubernetes requires that you disable the swap memory in the host system because the kubernetes scheduler determines the best available node on which to deploy newly created pods. If memory swapping is allowed to occur on a host system, this can lead to performance and stability issues within Kubernetes</p>\n<p>You can disable the swap memory by deleting or commenting the swap entry in  <code>/etc/fstab</code>  manually or using the  <code>sed</code>  command</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master$ sudo swapoff -a &amp;&amp; sudo sed -i <span class=\"string\">&#x27;/ swap / s/^\\(.*\\)$/#\\1/g&#x27;</span> /etc/fstab</span><br></pre></td></tr></table></figure>\n<p>This command disbales the swap memory and comments out the swap entry in  <code>/etc/fstab</code></p>\n<h3 id=\"configure-or-disable-the-firewall\"><a class=\"markdownIt-Anchor\" href=\"#configure-or-disable-the-firewall\">#</a> Configure or Disable the firewall</h3>\n<p>When running Kubernetes in an environment with strict network boundaries, such as on-premises datacenter with physical network firewalls or Virtual Networks in Public Cloud, it is useful to be aware of the ports and protocols used by Kubernetes components.</p>\n<p>The ports used by Master Node:</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Direction</th>\n<th>Port Range</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>6443</td>\n<td>Kubernetes API server</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>2379-2380</td>\n<td>etcd server client API</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>10250</td>\n<td>Kubelet API</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>10259</td>\n<td>kube-scheduler</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>10257</td>\n<td>kube-controller-manager</td>\n</tr>\n</tbody>\n</table>\n<p>The ports used by Worker Nodes:</p>\n<table>\n<thead>\n<tr>\n<th>Protocol</th>\n<th>Direction</th>\n<th>Port Range</th>\n<th>Purpose</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>10250</td>\n<td>Kubelet API</td>\n</tr>\n<tr>\n<td>TCP</td>\n<td>Inbound</td>\n<td>30000-32767</td>\n<td>NodePort Services</td>\n</tr>\n</tbody>\n</table>\n<p>You can either disable the firewall or allow the ports on each node.</p>\n<h4 id=\"method-1-add-firewall-rules-to-allow-the-ports-used-by-the-kubernetes-nodes\"><a class=\"markdownIt-Anchor\" href=\"#method-1-add-firewall-rules-to-allow-the-ports-used-by-the-kubernetes-nodes\">#</a> Method 1: Add firewall rules to allow the ports used by the Kubernetes nodes</h4>\n<p>Allow the ports used by the master node:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw allow 6443/tcp</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw allow 2379:2380/tcp</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw allow 10250/tcp</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw allow 10259/tcp</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw allow 10257/tcp</span><br></pre></td></tr></table></figure>\n<p>Allow the ports used by the worker nodes:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ sudo ufw allow 10250/tcp</span><br><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ sudo ufw allow 30000:32767/tcp</span><br></pre></td></tr></table></figure>\n<h4 id=\"method-2-disable-the-firewall\"><a class=\"markdownIt-Anchor\" href=\"#method-2-disable-the-firewall\">#</a> Method 2: Disable the firewall</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt command</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw status</span><br><span class=\"line\">Status: active</span><br><span class=\"line\"></span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw <span class=\"built_in\">disable</span></span><br><span class=\"line\">Firewall stopped and disabled on system startup</span><br><span class=\"line\"></span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo ufw status</span><br><span class=\"line\">Status: inactive</span><br></pre></td></tr></table></figure>\n<h3 id=\"installing-docker-engine\"><a class=\"markdownIt-Anchor\" href=\"#installing-docker-engine\">#</a> Installing Docker Engine</h3>\n<p>Kubernetes requires you to install a container runtime to work correctly.There are many available options like containerd, CRI-O, Docker etc</p>\n<p>By default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.If you don’t specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.</p>\n<p>You must install the Docker Engine on each node!</p>\n<h4 id=\"1-set-up-the-repository\"><a class=\"markdownIt-Anchor\" href=\"#1-set-up-the-repository\">#</a> 1- Set up the repository</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo apt update</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo apt install ca-certificates curl gnupg lsb-release</span><br></pre></td></tr></table></figure>\n<h4 id=\"2-add-dockers-official-gpg-key\"><a class=\"markdownIt-Anchor\" href=\"#2-add-dockers-official-gpg-key\">#</a> 2- Add Docker’s official GPG key</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo <span class=\"built_in\">mkdir</span> -p /etc/apt/keyrings</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg</span><br></pre></td></tr></table></figure>\n<h4 id=\"3-add-the-stable-repository-using-the-following-command\"><a class=\"markdownIt-Anchor\" href=\"#3-add-the-stable-repository-using-the-following-command\">#</a> 3- Add the stable repository using the following command:</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ <span class=\"built_in\">echo</span> \\</span><br><span class=\"line\">  <span class=\"string\">&quot;deb [arch=<span class=\"subst\">$(dpkg --print-architecture)</span> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\</span></span><br><span class=\"line\"><span class=\"string\">  <span class=\"subst\">$(lsb_release -cs)</span> stable&quot;</span> | sudo <span class=\"built_in\">tee</span> /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br></pre></td></tr></table></figure>\n<h4 id=\"4-install-the-docker-container\"><a class=\"markdownIt-Anchor\" href=\"#4-install-the-docker-container\">#</a> 4- Install the docker container</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo apt update &amp;&amp; sudo apt install docker-ce docker-ce-cli containerd.io -y</span><br></pre></td></tr></table></figure>\n<h4 id=\"5-make-sure-that-the-docker-will-work-on-system-startup\"><a class=\"markdownIt-Anchor\" href=\"#5-make-sure-that-the-docker-will-work-on-system-startup\">#</a> 5- Make sure that the docker will work on system startup</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo systemctl <span class=\"built_in\">enable</span> --now docker </span><br></pre></td></tr></table></figure>\n<h4 id=\"6-configuring-cgroup-driver\"><a class=\"markdownIt-Anchor\" href=\"#6-configuring-cgroup-driver\">#</a> 6- Configuring Cgroup Driver:</h4>\n<p>The Cgroup Driver must be configured to let the kubelet process work correctly</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ <span class=\"built_in\">cat</span> &lt;&lt;<span class=\"string\">EOF | sudo tee /etc/docker/daemon.json</span></span><br><span class=\"line\"><span class=\"string\">&#123;</span></span><br><span class=\"line\"><span class=\"string\">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span></span><br><span class=\"line\"><span class=\"string\">  &quot;log-driver&quot;: &quot;json-file&quot;,</span></span><br><span class=\"line\"><span class=\"string\">  &quot;log-opts&quot;: &#123;</span></span><br><span class=\"line\"><span class=\"string\">    &quot;max-size&quot;: &quot;100m&quot;</span></span><br><span class=\"line\"><span class=\"string\">  &#125;,</span></span><br><span class=\"line\"><span class=\"string\">  &quot;storage-driver&quot;: &quot;overlay2&quot;</span></span><br><span class=\"line\"><span class=\"string\">&#125;</span></span><br><span class=\"line\"><span class=\"string\">EOF</span></span><br></pre></td></tr></table></figure>\n<h4 id=\"7-restart-the-docker-service-to-make-sure-the-new-configuration-is-applied\"><a class=\"markdownIt-Anchor\" href=\"#7-restart-the-docker-service-to-make-sure-the-new-configuration-is-applied\">#</a> 7- Restart the docker service to make sure the new configuration is applied</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker</span><br></pre></td></tr></table></figure>\n<h3 id=\"installing-kubernetes-kubeadm-kubelet-and-kubectl\"><a class=\"markdownIt-Anchor\" href=\"#installing-kubernetes-kubeadm-kubelet-and-kubectl\">#</a> Installing kubernetes (kubeadm, kubelet, and kubectl):</h3>\n<p>Install the following dependency required by Kubernetes on each node</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo apt install apt-transport-https</span><br></pre></td></tr></table></figure>\n<h4 id=\"download-the-google-cloud-public-signing-key\"><a class=\"markdownIt-Anchor\" href=\"#download-the-google-cloud-public-signing-key\">#</a> Download the Google Cloud public signing key:</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</span><br></pre></td></tr></table></figure>\n<h4 id=\"add-the-kubernetes-apt-repository\"><a class=\"markdownIt-Anchor\" href=\"#add-the-kubernetes-apt-repository\">#</a> Add the Kubernetes apt repository:</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ <span class=\"built_in\">echo</span> <span class=\"string\">&quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&quot;</span> | sudo <span class=\"built_in\">tee</span> /etc/apt/sources.list.d/kubernetes.list</span><br></pre></td></tr></table></figure>\n<h4 id=\"update-the-apt-package-index-and-install-kubeadm-kubelet-and-kubeclt\"><a class=\"markdownIt-Anchor\" href=\"#update-the-apt-package-index-and-install-kubeadm-kubelet-and-kubeclt\">#</a> Update the apt package index and install kubeadm, kubelet, and kubeclt</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo apt update &amp;&amp; sudo apt install -y kubelet=1.23.1-00 kubectl=1.23.1-00 kubeadm=1.23.1-00</span><br></pre></td></tr></table></figure>\n<h2 id=\"initializing-the-control-plane-node\"><a class=\"markdownIt-Anchor\" href=\"#initializing-the-control-plane-node\">#</a> Initializing the control-plane node</h2>\n<p>At this point, we have 3 nodes with docker,  <code>kubeadm</code> ,  <code>kubelet</code> , and  <code>kubectl</code>  installed. Now we must initialize the Kubernetes master, which will manage the whole cluster and the pods running within the cluster  <code>kubeadm init</code>  by specifiy the address of the master node and the ipv4 address pool of the pods</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo kubeadm init --apiserver-advertise-address=192.168.1.18 --pod-network-cidr=10.1.0.0/16</span><br></pre></td></tr></table></figure>\n<p>You should wait a few minutes until the initialization is completed. The first initialization will take a lot of time if your connexion speed is slow (pull the images of the cluster components)</p>\n<h3 id=\"configuring-kubectl\"><a class=\"markdownIt-Anchor\" href=\"#configuring-kubectl\">#</a> Configuring kubectl</h3>\n<p>As known, the  <code>kubectl</code>  is a command line tool for performing actions on your cluster. So we must to configure  <code>kubectl</code> . Run the following command from your master node:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ <span class=\"built_in\">mkdir</span> -p <span class=\"variable\">$HOME</span>/.kube</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo <span class=\"built_in\">cp</span> -i /etc/kubernetes/admin.conf <span class=\"variable\">$HOME</span>/.kube/config</span><br><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo <span class=\"built_in\">chown</span> $(<span class=\"built_in\">id</span> -u):$(<span class=\"built_in\">id</span> -g) <span class=\"variable\">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>\n<h3 id=\"installing-calico-cni\"><a class=\"markdownIt-Anchor\" href=\"#installing-calico-cni\">#</a> Installing Calico CNI</h3>\n<p>Calico provides network and network security solutions for containers. Calico is best known for its performance, flexibility and power. Use-cases: Calico can be used within a lot of Kubernetes platforms (kops, Kubespray, docker enterprise, etc.) to block or allow traffic between pods, namespaces</p>\n<h4 id=\"1-install-tigera-calico-operator\"><a class=\"markdownIt-Anchor\" href=\"#1-install-tigera-calico-operator\">#</a> 1- Install Tigera Calico operator</h4>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl create -f <span class=\"string\">&quot;https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml&quot;</span></span><br></pre></td></tr></table></figure>\n<p>The Tigera Operator is a Kubernetes operator which manages the lifecycle of a Calico or Calico Enterprise installation on Kubernetes. Its goal is to make installation, upgrades, and ongoing lifecycle management of Calico and Calico Enterprise as simple and reliable as possible.</p>\n<h4 id=\"2-download-the-custom-resourcesyaml-manifest-and-change-it\"><a class=\"markdownIt-Anchor\" href=\"#2-download-the-custom-resourcesyaml-manifest-and-change-it\">#</a> 2- Download the custom-resources.yaml manifest and change it</h4>\n<p>The Calico has a default pod’s CIDR value. But in our example, we set the   <code>--pod-netwokr-cidr=10.1.0.0/16</code> . So we must change the value of pod network CIDR in  <code>custom-resources.yaml</code></p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ wget  <span class=\"string\">&quot;https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml&quot;</span></span><br></pre></td></tr></table></figure>\n<p>Now we edit this file before create the Calico pods</p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># This section includes base Calico installation configuration.</span></span><br><span class=\"line\"><span class=\"comment\"># For more information, see: https://projectcalico.docs.tigera.io/v3.23/reference/installation/api#operator.tigera.io/v1.Installation</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">operator.tigera.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">Installation</span></span><br><span class=\"line\"><span class=\"attr\">metadata:</span></span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">default</span></span><br><span class=\"line\"><span class=\"attr\">spec:</span></span><br><span class=\"line\">  <span class=\"comment\"># Configures Calico networking.</span></span><br><span class=\"line\">  <span class=\"attr\">calicoNetwork:</span></span><br><span class=\"line\">    <span class=\"comment\"># <span class=\"doctag\">Note:</span> The ipPools section cannot be modified post-install.</span></span><br><span class=\"line\">    <span class=\"attr\">ipPools:</span></span><br><span class=\"line\">    <span class=\"bullet\">-</span> <span class=\"attr\">blockSize:</span> <span class=\"number\">26</span></span><br><span class=\"line\">      <span class=\"attr\">cidr:</span> <span class=\"number\">10.1</span><span class=\"number\">.0</span><span class=\"number\">.0</span><span class=\"string\">/16</span> <span class=\"comment\">#change this value with yours</span></span><br><span class=\"line\">      <span class=\"attr\">encapsulation:</span> <span class=\"string\">VXLANCrossSubnet</span></span><br><span class=\"line\">      <span class=\"attr\">natOutgoing:</span> <span class=\"string\">Enabled</span></span><br><span class=\"line\">      <span class=\"attr\">nodeSelector:</span> <span class=\"string\">all()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">---</span></span><br><span class=\"line\"><span class=\"meta\"></span></span><br><span class=\"line\"><span class=\"comment\"># This section configures the Calico API server.</span></span><br><span class=\"line\"><span class=\"comment\"># For more information, see: https://projectcalico.docs.tigera.io/v3.23/reference/installation/api#operator.tigera.io/v1.APIServer</span></span><br><span class=\"line\"><span class=\"attr\">apiVersion:</span> <span class=\"string\">operator.tigera.io/v1</span></span><br><span class=\"line\"><span class=\"attr\">kind:</span> <span class=\"string\">APIServer</span> </span><br><span class=\"line\"><span class=\"attr\">metadata:</span> </span><br><span class=\"line\">  <span class=\"attr\">name:</span> <span class=\"string\">default</span> </span><br><span class=\"line\"><span class=\"attr\">spec:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>\n<p>After Editing the  <code>custom-resources.yaml</code>  file. Run the following command:</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl create -f <span class=\"string\">&quot;custom-resources.yaml&quot;</span> </span><br></pre></td></tr></table></figure>\n<p>Before you can use the cluster, you must wait for the pods required by Calico to be downloaded. You must wait until you find all the pods running and ready!</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl get pods --all-namespaces</span><br><span class=\"line\">NAMESPACE          NAME                                       READY   STATUS    RESTARTS       AGE</span><br><span class=\"line\">calico-apiserver   calico-apiserver-5989576d6d-5nw7n          1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">calico-apiserver   calico-apiserver-5989576d6d-h677h          1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">calico-system      calico-kube-controllers-69cfd64db4-9hnh5   1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">calico-system      calico-node-lshdl                          1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">calico-system      calico-typha-76dd7c96d7-88826              1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        coredns-64897985d-jkpwh                    1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        coredns-64897985d-zk9wx                    1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        etcd-master                                1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        kube-apiserver-master                      1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        kube-controller-manager-master             1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        kube-proxy-4nf4q                           1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">kube-system        kube-scheduler-master                      1/1     Running   1 (4min ago)    4min</span><br><span class=\"line\">tigera-operator    tigera-operator-7d8c9d4f67-j5b2g           1/1     Running   2 (103s ago)    4min</span><br></pre></td></tr></table></figure>\n<h2 id=\"join-the-worker-nodes\"><a class=\"markdownIt-Anchor\" href=\"#join-the-worker-nodes\">#</a> Join the worker nodes</h2>\n<p>Now our cluster is ready to work! let’s join the worker nodes to this cluster by getting the token from the master node</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ sudo kubeadm token create --print-join-command</span><br><span class=\"line\">kubeadm <span class=\"built_in\">join</span> 192.168.1.18:6443 --token g4mgtb.e8zgs1c0kpkaj9wt --discovery-token-ca-cert-hash sha256:047628de2a0a43127b7c4774093203631d315451874efc6b63421a4da9bee2ec</span><br></pre></td></tr></table></figure>\n<p>Now let’s move to the worker node and run the following command given by  <code>kubeadm token create</code></p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-worker1:~$ sudo kubeadm <span class=\"built_in\">join</span> 192.168.1.18:6443 --token g4mgtb.e8zgs1c0kpkaj9wt </span><br><span class=\"line\">\\--discovery-token-ca-cert-hash sha256:047628de2a0a43127b7c4774093203631d315451874efc6b63421a4da9bee2ec</span><br></pre></td></tr></table></figure>\n<p>The output must be similar to the following</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[preflight] Running pre-flight checks</span><br><span class=\"line\">[preflight] Reading configuration from the cluster...</span><br><span class=\"line\">[preflight] FYI: You can look at this config file with <span class=\"string\">&#x27;kubectl -n kube-system get cm kubeadm-config -o yaml&#x27;</span></span><br><span class=\"line\">W0623 12:45:07.940655   23651 utils.go:69] The recommended value <span class=\"keyword\">for</span> <span class=\"string\">&quot;resolvConf&quot;</span> <span class=\"keyword\">in</span> <span class=\"string\">&quot;KubeletConfiguration&quot;</span> is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf</span><br><span class=\"line\">[kubelet-start] Writing kubelet configuration to file <span class=\"string\">&quot;/var/lib/kubelet/config.yaml&quot;</span></span><br><span class=\"line\">[kubelet-start] Writing kubelet environment file with flags to file <span class=\"string\">&quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span></span><br><span class=\"line\">[kubelet-start] Starting the kubelet</span><br><span class=\"line\">[kubelet-start] Waiting <span class=\"keyword\">for</span> the kubelet to perform the TLS Bootstrap...</span><br><span class=\"line\"></span><br><span class=\"line\">This node has joined the cluster:</span><br><span class=\"line\">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class=\"line\">* The Kubelet was informed of the new secure connection details.</span><br><span class=\"line\"></span><br><span class=\"line\">Run <span class=\"string\">&#x27;kubectl get nodes&#x27;</span> on the control-plane to see this node <span class=\"built_in\">join</span> the cluster.</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Now let’s Check the cluster by running  <code>kubectl get nodes</code>  command on the master node.</p>\n<figure class=\"highlight bash\"><figcaption><span>command line prompt</span></figcaption><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">4n6nk8s@4n6nk8s-master:~$ kubectl get nodes</span><br><span class=\"line\"></span><br><span class=\"line\">NAME              STATUS     ROLES                  AGE    VERSION</span><br><span class=\"line\">4n6nk8s-master    Ready      control-plane,master   40m5s  v1.23.1</span><br><span class=\"line\">4n6nk8s-worker1   Ready      &lt;none&gt;                 3m7s   v1.23.1</span><br><span class=\"line\">4n6nk8s-worker2   Ready      &lt;none&gt;                 2m3s   v1.23.1</span><br></pre></td></tr></table></figure>\n<h2 id=\"references\"><a class=\"markdownIt-Anchor\" href=\"#references\">#</a> References:</h2>\n<div class=\"links\"><div class=\"item\" title=\"Kubernetes Documentation\" style=\"--block-color:#2296fd;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3Mvc2V0dXAvcHJvZHVjdGlvbi1lbnZpcm9ubWVudC90b29scy9rdWJlYWRtL2NyZWF0ZS1jbHVzdGVyLWt1YmVhZG0v\" data-background-image=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/39/Kubernetes_logo_without_workmark.svg/1200px-Kubernetes_logo_without_workmark.svg.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3Mvc2V0dXAvcHJvZHVjdGlvbi1lbnZpcm9ubWVudC90b29scy9rdWJlYWRtL2NyZWF0ZS1jbHVzdGVyLWt1YmVhZG0v\">Kubernetes Documentation</span>\n          <p class=\"desc\">Creating a cluster with kubeadm</p>\n          </div></div><div class=\"item\" title=\"Calico Documentation\" style=\"--block-color:#FB7031;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9wcm9qZWN0Y2FsaWNvLmRvY3MudGlnZXJhLmlvL2dldHRpbmctc3RhcnRlZC9rdWJlcm5ldGVzL3NlbGYtbWFuYWdlZC1vbnByZW0vb25wcmVtaXNlcw==\" data-background-image=\"https://projectcalico.docs.tigera.io/images/favicon.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9wcm9qZWN0Y2FsaWNvLmRvY3MudGlnZXJhLmlvL2dldHRpbmctc3RhcnRlZC9rdWJlcm5ldGVzL3NlbGYtbWFuYWdlZC1vbnByZW0vb25wcmVtaXNlcw==\">Calico Documentation</span>\n          <p class=\"desc\">Install Calico Networking for on-premises deployments</p>\n          </div></div><div class=\"item\" title=\"Docker Documentation\" style=\"--block-color:#31bffb;\"><span class=\"exturl image\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZW5naW5lL2luc3RhbGwvdWJ1bnR1Lw==\" data-background-image=\"https://www.docker.com/wp-content/uploads/2022/05/Docker_Temporary_Image_Google_Blue_1080x1080_v1.png\"></span>\n          <div class=\"info\">\n          <span class=\"exturl title\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZW5naW5lL2luc3RhbGwvdWJ1bnR1Lw==\">Docker Documentation</span>\n          <p class=\"desc\">Install Docker Engine on Ubuntu</p>\n          </div></div></div>\n",
            "tags": []
        }
    ]
}